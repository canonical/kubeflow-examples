{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Twitter sentiment analysis using AutoML"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\r\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.8/62.8 kB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting tabulate\r\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Collecting future\r\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m840.9/840.9 kB\u001B[0m \u001B[31m8.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting matplotlib\r\n",
      "  Downloading matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.6/11.6 MB\u001B[0m \u001B[31m11.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting certifi>=2017.4.17\r\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\r\n",
      "Collecting urllib3<1.27,>=1.21.1\r\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m140.6/140.6 kB\u001B[0m \u001B[31m11.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: idna<4,>=2.5 in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (from requests) (3.4)\r\n",
      "Collecting charset-normalizer<4,>=2\r\n",
      "  Downloading charset_normalizer-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m198.8/198.8 kB\u001B[0m \u001B[31m10.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (from matplotlib) (23.0)\r\n",
      "Collecting pillow>=6.2.0\r\n",
      "  Downloading Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m12.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting fonttools>=4.22.0\r\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\r\n",
      "Collecting cycler>=0.10\r\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\r\n",
      "Collecting kiwisolver>=1.0.1\r\n",
      "  Using cached kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\r\n",
      "Collecting contourpy>=1.0.1\r\n",
      "  Downloading contourpy-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m300.3/300.3 kB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting pyparsing>=2.3.1\r\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\r\n",
      "Collecting numpy>=1.20\r\n",
      "  Downloading numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m17.3/17.3 MB\u001B[0m \u001B[31m12.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "Building wheels for collected packages: future\r\n",
      "  Building wheel for future (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492025 sha256=e105b63ac5f7024f317f4580a4929d277e88ed6c52a75f88e7f83d31a83e8a33\r\n",
      "  Stored in directory: /home/barteus/.cache/pip/wheels/69/c0/ce/f2a18105d619f21239a048bcc58e98d8ce47ac824e0531f1a0\r\n",
      "Successfully built future\r\n",
      "Installing collected packages: charset-normalizer, urllib3, tabulate, pyparsing, pillow, numpy, kiwisolver, future, fonttools, cycler, certifi, requests, contourpy, matplotlib\r\n",
      "Successfully installed certifi-2022.12.7 charset-normalizer-3.0.1 contourpy-1.0.7 cycler-0.11.0 fonttools-4.38.0 future-0.18.3 kiwisolver-1.4.4 matplotlib-3.7.0 numpy-1.24.2 pillow-9.4.0 pyparsing-3.0.9 requests-2.28.2 tabulate-0.9.0 urllib3-1.26.14\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install requests tabulate future matplotlib"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html\r\n",
      "Requirement already satisfied: h2o in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (3.40.0.1)\r\n",
      "Requirement already satisfied: tabulate in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (from h2o) (0.9.0)\r\n",
      "Requirement already satisfied: requests in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (from h2o) (2.28.2)\r\n",
      "Requirement already satisfied: future in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (from h2o) (0.18.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (from requests->h2o) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (from requests->h2o) (1.26.14)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (from requests->h2o) (3.0.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages (from requests->h2o) (2022.12.7)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"11.0.17\" 2022-10-18; OpenJDK Runtime Environment (build 11.0.17+8-post-Ubuntu-1ubuntu222.04); OpenJDK 64-Bit Server VM (build 11.0.17+8-post-Ubuntu-1ubuntu222.04, mixed mode, sharing)\n",
      "  Starting server from /home/barteus/Work/DSV/kubeflow-examples/venv2/lib/python3.10/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpt823bdxl\n",
      "  JVM stdout: /tmp/tmpt823bdxl/h2o_barteus_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpt823bdxl/h2o_barteus_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/plain": "--------------------------  ------------------------------\nH2O_cluster_uptime:         01 secs\nH2O_cluster_timezone:       Europe/Warsaw\nH2O_data_parsing_timezone:  UTC\nH2O_cluster_version:        3.40.0.1\nH2O_cluster_version_age:    18 days\nH2O_cluster_name:           H2O_from_python_barteus_9iyi2m\nH2O_cluster_total_nodes:    1\nH2O_cluster_free_memory:    7.770 Gb\nH2O_cluster_total_cores:    8\nH2O_cluster_allowed_cores:  8\nH2O_cluster_status:         locked, healthy\nH2O_connection_url:         http://127.0.0.1:54321\nH2O_connection_proxy:       {\"http\": null, \"https\": null}\nH2O_internal_security:      False\nPython_version:             3.10.6 final\n--------------------------  ------------------------------",
      "text/html": "\n<style>\n\n#h2o-table-1.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-1 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-1 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-1 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-1 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-1 .h2o-table th,\n#h2o-table-1 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-1 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-1\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption></caption>\n    <thead></thead>\n    <tbody><tr><td>H2O_cluster_uptime:</td>\n<td>01 secs</td></tr>\n<tr><td>H2O_cluster_timezone:</td>\n<td>Europe/Warsaw</td></tr>\n<tr><td>H2O_data_parsing_timezone:</td>\n<td>UTC</td></tr>\n<tr><td>H2O_cluster_version:</td>\n<td>3.40.0.1</td></tr>\n<tr><td>H2O_cluster_version_age:</td>\n<td>18 days</td></tr>\n<tr><td>H2O_cluster_name:</td>\n<td>H2O_from_python_barteus_9iyi2m</td></tr>\n<tr><td>H2O_cluster_total_nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O_cluster_free_memory:</td>\n<td>7.770 Gb</td></tr>\n<tr><td>H2O_cluster_total_cores:</td>\n<td>8</td></tr>\n<tr><td>H2O_cluster_allowed_cores:</td>\n<td>8</td></tr>\n<tr><td>H2O_cluster_status:</td>\n<td>locked, healthy</td></tr>\n<tr><td>H2O_connection_url:</td>\n<td>http://127.0.0.1:54321</td></tr>\n<tr><td>H2O_connection_proxy:</td>\n<td>{\"http\": null, \"https\": null}</td></tr>\n<tr><td>H2O_internal_security:</td>\n<td>False</td></tr>\n<tr><td>Python_version:</td>\n<td>3.10.6 final</td></tr></tbody>\n  </table>\n</div>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "h2o.init()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "df = h2o.import_file(\"https://raw.githubusercontent.com/choas/h2o-titanic/master/data/train.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 rows removed\n"
     ]
    }
   ],
   "source": [
    "nrows_prev = df.nrows\n",
    "not_empty_embarked = ~df[\"Embarked\"].isna()\n",
    "df_e = df[not_empty_embarked]\n",
    "not_empty_age = df_e[\"Age\"] >= 0\n",
    "df = df_e[not_empty_age]\n",
    "print(\"%d rows removed\" % (nrows_prev - df.nrows))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# filter unknown Age and split 70% train and 30% test\n",
    "train, test = df[df[\"Age\"] >= 0].split_frame(ratios=[.7])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Identify predictors and response\n",
    "x = train.columns\n",
    "y = \"Survived\"\n",
    "x.remove(y)\n",
    "\n",
    "# remove some columns\n",
    "x.remove(\"PassengerId\")\n",
    "x.remove(\"Name\")\n",
    "x.remove(\"Ticket\")\n",
    "x.remove(\"Cabin\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# For binary classification, response should be a factor\n",
    "train[y] = train[y].asfactor()\n",
    "test[y] = test[y].asfactor()\n",
    "\n",
    "train[\"Pclass\"] = train[\"Pclass\"].asfactor()\n",
    "test[\"Pclass\"] = test[\"Pclass\"].asfactor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26606/3201333431.py:1: H2ODeprecationWarning: ``summary()`` is deprecated, please use ``show_summary()`` or ``get_summary()`` instead\n",
      "  train.summary()\n"
     ]
    },
    {
     "data": {
      "text/plain": "         PassengerId         Survived    Pclass    Name                                                 Sex     Age                 SibSp               Parch                Ticket             Fare                Cabin    Embarked\n-------  ------------------  ----------  --------  ---------------------------------------------------  ------  ------------------  ------------------  -------------------  -----------------  ------------------  -------  ----------\ntype     int                 enum        enum      string                                               enum    real                int                 int                  int                real                enum     enum\nmins     1.0                                       NaN                                                          0.42                0.0                 0.0                  693.0              0.0\nmean     450.40718562874224                        NaN                                                          29.549241516966077  0.5449101796407183  0.41916167664670684  287598.5382513659  35.105114570858255\nmaxs     891.0                                     NaN                                                          80.0                5.0                 6.0                  3101296.0          512.3292\nsigma    256.93821410655045                        NaN                                                          14.384895182918536  0.9800403266785341  0.8740435319870421   537962.9089762091  56.60094303009492\nzeros    0                                         0                                                            0                   327                 374                  0                  5\nmissing  0                   0           0         0                                                    0       0                   0                   0                    135                0                   375      0\n0        1.0                 0           3         Braund, Mr. Owen Harris                              male    22.0                1.0                 0.0                  nan                7.25                         S\n1        2.0                 1           1         Cumings, Mrs. John Bradley (Florence Briggs Thayer)  female  38.0                1.0                 0.0                  nan                71.2833             C85      C\n2        3.0                 1           3         Heikkinen, Miss. Laina                               female  26.0                0.0                 0.0                  nan                7.925                        S\n3        4.0                 1           1         Futrelle, Mrs. Jacques Heath (Lily May Peel)         female  35.0                1.0                 0.0                  113803.0           53.1                C123     S\n4        7.0                 0           1         McCarthy, Mr. Timothy J                              male    54.0                0.0                 0.0                  17463.0            51.8625             E46      S\n5        9.0                 1           3         Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    female  27.0                0.0                 2.0                  347742.0           11.1333                      S\n6        10.0                1           2         Nasser, Mrs. Nicholas (Adele Achem)                  female  14.0                1.0                 0.0                  237736.0           30.0708                      C\n7        13.0                0           3         Saundercock, Mr. William Henry                       male    20.0                0.0                 0.0                  nan                8.05                         S\n8        14.0                0           3         Andersson, Mr. Anders Johan                          male    39.0                1.0                 5.0                  347082.0           31.275                       S\n9        16.0                1           2         Hewlett, Mrs. (Mary D Kingcome)                      female  55.0                0.0                 0.0                  248706.0           16.0                         S\n[501 rows x 12 columns]\n",
      "text/html": "<table class='dataframe'>\n<thead>\n<tr><th>       </th><th>PassengerId       </th><th>Survived  </th><th>Pclass  </th><th>Name                                               </th><th>Sex   </th><th>Age               </th><th>SibSp             </th><th>Parch              </th><th>Ticket           </th><th>Fare              </th><th>Cabin  </th><th>Embarked  </th></tr>\n</thead>\n<tbody>\n<tr><td>type   </td><td>int               </td><td>enum      </td><td>enum    </td><td>string                                             </td><td>enum  </td><td>real              </td><td>int               </td><td>int                </td><td>int              </td><td>real              </td><td>enum   </td><td>enum      </td></tr>\n<tr><td>mins   </td><td>1.0               </td><td>          </td><td>        </td><td>NaN                                                </td><td>      </td><td>0.42              </td><td>0.0               </td><td>0.0                </td><td>693.0            </td><td>0.0               </td><td>       </td><td>          </td></tr>\n<tr><td>mean   </td><td>450.40718562874224</td><td>          </td><td>        </td><td>NaN                                                </td><td>      </td><td>29.549241516966077</td><td>0.5449101796407183</td><td>0.41916167664670684</td><td>287598.5382513659</td><td>35.105114570858255</td><td>       </td><td>          </td></tr>\n<tr><td>maxs   </td><td>891.0             </td><td>          </td><td>        </td><td>NaN                                                </td><td>      </td><td>80.0              </td><td>5.0               </td><td>6.0                </td><td>3101296.0        </td><td>512.3292          </td><td>       </td><td>          </td></tr>\n<tr><td>sigma  </td><td>256.93821410655045</td><td>          </td><td>        </td><td>NaN                                                </td><td>      </td><td>14.384895182918536</td><td>0.9800403266785341</td><td>0.8740435319870421 </td><td>537962.9089762091</td><td>56.60094303009492 </td><td>       </td><td>          </td></tr>\n<tr><td>zeros  </td><td>0                 </td><td>          </td><td>        </td><td>0                                                  </td><td>      </td><td>0                 </td><td>327               </td><td>374                </td><td>0                </td><td>5                 </td><td>       </td><td>          </td></tr>\n<tr><td>missing</td><td>0                 </td><td>0         </td><td>0       </td><td>0                                                  </td><td>0     </td><td>0                 </td><td>0                 </td><td>0                  </td><td>135              </td><td>0                 </td><td>375    </td><td>0         </td></tr>\n<tr><td>0      </td><td>1.0               </td><td>0         </td><td>3       </td><td>Braund, Mr. Owen Harris                            </td><td>male  </td><td>22.0              </td><td>1.0               </td><td>0.0                </td><td>nan              </td><td>7.25              </td><td>       </td><td>S         </td></tr>\n<tr><td>1      </td><td>2.0               </td><td>1         </td><td>1       </td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38.0              </td><td>1.0               </td><td>0.0                </td><td>nan              </td><td>71.2833           </td><td>C85    </td><td>C         </td></tr>\n<tr><td>2      </td><td>3.0               </td><td>1         </td><td>3       </td><td>Heikkinen, Miss. Laina                             </td><td>female</td><td>26.0              </td><td>0.0               </td><td>0.0                </td><td>nan              </td><td>7.925             </td><td>       </td><td>S         </td></tr>\n<tr><td>3      </td><td>4.0               </td><td>1         </td><td>1       </td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)       </td><td>female</td><td>35.0              </td><td>1.0               </td><td>0.0                </td><td>113803.0         </td><td>53.1              </td><td>C123   </td><td>S         </td></tr>\n<tr><td>4      </td><td>7.0               </td><td>0         </td><td>1       </td><td>McCarthy, Mr. Timothy J                            </td><td>male  </td><td>54.0              </td><td>0.0               </td><td>0.0                </td><td>17463.0          </td><td>51.8625           </td><td>E46    </td><td>S         </td></tr>\n<tr><td>5      </td><td>9.0               </td><td>1         </td><td>3       </td><td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  </td><td>female</td><td>27.0              </td><td>0.0               </td><td>2.0                </td><td>347742.0         </td><td>11.1333           </td><td>       </td><td>S         </td></tr>\n<tr><td>6      </td><td>10.0              </td><td>1         </td><td>2       </td><td>Nasser, Mrs. Nicholas (Adele Achem)                </td><td>female</td><td>14.0              </td><td>1.0               </td><td>0.0                </td><td>237736.0         </td><td>30.0708           </td><td>       </td><td>C         </td></tr>\n<tr><td>7      </td><td>13.0              </td><td>0         </td><td>3       </td><td>Saundercock, Mr. William Henry                     </td><td>male  </td><td>20.0              </td><td>0.0               </td><td>0.0                </td><td>nan              </td><td>8.05              </td><td>       </td><td>S         </td></tr>\n<tr><td>8      </td><td>14.0              </td><td>0         </td><td>3       </td><td>Andersson, Mr. Anders Johan                        </td><td>male  </td><td>39.0              </td><td>1.0               </td><td>5.0                </td><td>347082.0         </td><td>31.275            </td><td>       </td><td>S         </td></tr>\n<tr><td>9      </td><td>16.0              </td><td>1         </td><td>2       </td><td>Hewlett, Mrs. (Mary D Kingcome)                    </td><td>female</td><td>55.0              </td><td>0.0               </td><td>0.0                </td><td>248706.0         </td><td>16.0              </td><td>       </td><td>S         </td></tr>\n</tbody>\n</table><pre style='font-size: smaller; margin-bottom: 1em;'>[501 rows x 12 columns]</pre>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |\n",
      "11:03:17.962: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
      "\n",
      "███████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": "Model Details\n=============\nH2OStackedEnsembleEstimator : Stacked Ensemble\nModel Key: StackedEnsemble_BestOfFamily_3_AutoML_1_20230224_110317\n\n\nModel Summary for Stacked Ensemble: \nkey                                        value\n-----------------------------------------  ----------------\nStacking strategy                          cross_validation\nNumber of base models (used / total)       6/6\n# GBM base models (used / total)           1/1\n# XGBoost base models (used / total)       1/1\n# DRF base models (used / total)           2/2\n# GLM base models (used / total)           1/1\n# DeepLearning base models (used / total)  1/1\nMetalearner algorithm                      GLM\nMetalearner fold assignment scheme         Random\nMetalearner nfolds                         5\nMetalearner fold_column\nCustom metalearner hyperparameters         None\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.090302569991916\nRMSE: 0.3005038601947003\nLogLoss: 0.30788977805421525\nAUC: 0.9528632548955077\nAUCPR: 0.9442040059021868\nGini: 0.9057265097910154\nNull degrees of freedom: 500\nResidual degrees of freedom: 494\nNull deviance: 678.6388715052319\nResidual deviance: 308.5055576103236\nAIC: 322.5055576103236\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.48698351606560014\n       0    1    Error    Rate\n-----  ---  ---  -------  ------------\n0      282  13   0.0441   (13.0/295.0)\n1      39   167  0.1893   (39.0/206.0)\nTotal  321  180  0.1038   (52.0/501.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.486984     0.865285  155\nmax f2                       0.260119     0.899814  220\nmax f0point5                 0.549071     0.911162  145\nmax accuracy                 0.486984     0.896208  155\nmax precision                0.984631     1         0\nmax recall                   0.113078     1         320\nmax specificity              0.984631     1         0\nmax absolute_mcc             0.486984     0.786197  155\nmax min_per_class_accuracy   0.341544     0.864078  190\nmax mean_per_class_accuracy  0.486984     0.883306  155\nmax tns                      0.984631     295       0\nmax fns                      0.984631     205       0\nmax fps                      0.0271721    295       399\nmax tps                      0.113078     206       320\nmax tnr                      0.984631     1         0\nmax fnr                      0.984631     0.995146  0\nmax fpr                      0.0271721    1         399\nmax tpr                      0.113078     1         320\n\nGains/Lift Table: Avg response rate: 41.12 %, avg score: 40.55 %\ngroup    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.011976                    0.978779           2.43204    2.43204            1                0.980605   1                           0.980605            0.0291262       0.0291262                  143.204   143.204            0.0291262\n2        0.0219561                   0.972523           2.43204    2.43204            1                0.976001   1                           0.978513            0.0242718       0.0533981                  143.204   143.204            0.0533981\n3        0.0319361                   0.969986           2.43204    2.43204            1                0.970856   1                           0.97612             0.0242718       0.0776699                  143.204   143.204            0.0776699\n4        0.0419162                   0.968274           2.43204    2.43204            1                0.968814   1                           0.974381            0.0242718       0.101942                   143.204   143.204            0.101942\n5        0.0518962                   0.957717           2.43204    2.43204            1                0.963536   1                           0.972295            0.0242718       0.126214                   143.204   143.204            0.126214\n6        0.101796                    0.921165           2.43204    2.43204            1                0.944738   1                           0.958787            0.121359        0.247573                   143.204   143.204            0.247573\n7        0.151697                    0.882701           2.43204    2.43204            1                0.903015   1                           0.940441            0.121359        0.368932                   143.204   143.204            0.368932\n8        0.201597                    0.820844           2.43204    2.43204            1                0.859658   1                           0.920445            0.121359        0.490291                   143.204   143.204            0.490291\n9        0.301397                    0.649236           2.14019    2.3354             0.88             0.720786   0.960265                    0.854333            0.213592        0.703883                   114.019   133.54             0.683545\n10       0.401198                    0.407514           1.21602    2.05695            0.5              0.513314   0.845771                    0.769502            0.121359        0.825243                   21.6019   105.695            0.720158\n11       0.500998                    0.26092            1.0701     1.86036            0.44             0.32208    0.76494                     0.680374            0.106796        0.932039                   7.00971   86.0364            0.732039\n12       0.600798                    0.181872           0.340485   1.60789            0.14             0.219848   0.66113                     0.603875            0.0339806       0.966019                   -65.9515  60.7893            0.620257\n13       0.700599                    0.134256           0.243204   1.41349            0.1              0.160013   0.581197                    0.540647            0.0242718       0.990291                   -75.6796  41.3493            0.491986\n14       0.800399                    0.108409           0.0972816  1.24938            0.04             0.120925   0.513716                    0.488313            0.00970874      1                          -90.2718  24.9377            0.338983\n15       0.9002                      0.0748283          0          1.11086            0                0.0924047  0.456763                    0.44442             0               1                          -100      11.0865            0.169492\n16       1                           0.0271721          0          1                  0                0.0539791  0.411178                    0.405454            0               1                          -100      0                  0\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on validation data. **\n\nMSE: 0.10790103374848198\nRMSE: 0.3284829276362502\nLogLoss: 0.3641385099423577\nAUC: 0.9066458687842692\nAUCPR: 0.8922966766719541\nGini: 0.8132917375685385\nNull degrees of freedom: 210\nResidual degrees of freedom: 204\nNull deviance: 282.39644104064814\nResidual deviance: 153.66645119567494\nAIC: 167.66645119567494\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.5038318992954588\n       0    1    Error    Rate\n-----  ---  ---  -------  ------------\n0      117  12   0.093    (12.0/129.0)\n1      15   67   0.1829   (15.0/82.0)\nTotal  132  79   0.128    (27.0/211.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.503832     0.832298  75\nmax f2                       0.310263     0.87822   95\nmax f0point5                 0.583129     0.861582  65\nmax accuracy                 0.525517     0.872038  71\nmax precision                0.978893     1         0\nmax recall                   0.0593606    1         190\nmax specificity              0.978893     1         0\nmax absolute_mcc             0.503832     0.729239  75\nmax min_per_class_accuracy   0.403435     0.844961  86\nmax mean_per_class_accuracy  0.318286     0.865948  92\nmax tns                      0.978893     129       0\nmax fns                      0.978893     81        0\nmax fps                      0.0269644    129       204\nmax tps                      0.0593606    82        190\nmax tnr                      0.978893     1         0\nmax fnr                      0.978893     0.987805  0\nmax fpr                      0.0269644    1         204\nmax tpr                      0.0593606    1         190\n\nGains/Lift Table: Avg response rate: 38.86 %, avg score: 40.50 %\ngroup    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.014218                    0.972972           2.57317   2.57317            1                0.976732   1                           0.976732            0.0365854       0.0365854                  157.317   157.317            0.0365854\n2        0.0236967                   0.958303           2.57317   2.57317            1                0.963133   1                           0.971292            0.0243902       0.0609756                  157.317   157.317            0.0609756\n3        0.0331754                   0.956284           2.57317   2.57317            1                0.956506   1                           0.967068            0.0243902       0.0853659                  157.317   157.317            0.0853659\n4        0.042654                    0.954052           2.57317   2.57317            1                0.955704   1                           0.964542            0.0243902       0.109756                   157.317   157.317            0.109756\n5        0.0521327                   0.95015            2.57317   2.57317            1                0.95134    1                           0.962142            0.0243902       0.134146                   157.317   157.317            0.134146\n6        0.104265                    0.926348           2.33925   2.45621            0.909091         0.939118   0.954545                    0.95063             0.121951        0.256098                   133.925   145.621            0.248346\n7        0.151659                    0.895439           2.57317   2.49276            1                0.912399   0.96875                     0.938683            0.121951        0.378049                   157.317   149.276            0.370297\n8        0.203791                    0.738943           2.33925   2.45349            0.909091         0.853961   0.953488                    0.91701             0.121951        0.5                        133.925   145.349            0.484496\n9        0.303318                    0.617373           1.96051   2.29173            0.761905         0.689782   0.890625                    0.842451            0.195122        0.695122                   96.0511   129.173            0.640858\n10       0.402844                    0.428148           1.34785   2.05854            0.52381          0.526222   0.8                         0.764324            0.134146        0.829268                   34.7851   105.854            0.697485\n11       0.50237                     0.255878           0.857724  1.82064            0.333333         0.335746   0.707547                    0.679417            0.0853659       0.914634                   -14.2276  82.064             0.674324\n12       0.601896                    0.1869             0.122532  1.53985            0.047619         0.220772   0.598425                    0.603578            0.0121951       0.926829                   -87.7468  53.985             0.53148\n13       0.701422                    0.132084           0.122532  1.33874            0.047619         0.162607   0.52027                     0.541008            0.0121951       0.939024                   -87.7468  33.8744            0.388637\n14       0.800948                    0.100439           0.122532  1.18762            0.047619         0.11537    0.461538                    0.488118            0.0121951       0.95122                    -87.7468  18.7617            0.245793\n15       0.900474                    0.0727439          0.245064  1.08344            0.0952381        0.0858364  0.421053                    0.443655            0.0243902       0.97561                    -75.4936  8.34403            0.122897\n16       1                           0.0269644          0.245064  1                  0.0952381        0.0548286  0.388626                    0.404957            0.0243902       1                          -75.4936  0                  0\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on cross-validation data. **\n\nMSE: 0.14648277687501338\nRMSE: 0.38273068452243725\nLogLoss: 0.4593560459236398\nAUC: 0.8461329603422741\nAUCPR: 0.8292932326677792\nGini: 0.6922659206845483\nNull degrees of freedom: 500\nResidual degrees of freedom: 494\nNull deviance: 679.6192027459856\nResidual deviance: 460.2747580154872\nAIC: 474.2747580154872\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.42980900650465503\n       0    1    Error    Rate\n-----  ---  ---  -------  -------------\n0      241  54   0.1831   (54.0/295.0)\n1      46   160  0.2233   (46.0/206.0)\nTotal  287  214  0.1996   (100.0/501.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.429809     0.761905  189\nmax f2                       0.139081     0.809801  322\nmax f0point5                 0.752813     0.807692  91\nmax accuracy                 0.515095     0.808383  156\nmax precision                0.986813     1         0\nmax recall                   0.0316588    1         399\nmax specificity              0.986813     1         0\nmax absolute_mcc             0.515095     0.600191  156\nmax min_per_class_accuracy   0.379373     0.786408  199\nmax mean_per_class_accuracy  0.429809     0.796824  189\nmax tns                      0.986813     295       0\nmax fns                      0.986813     205       0\nmax fps                      0.0354786    295       398\nmax tps                      0.0316588    206       399\nmax tnr                      0.986813     1         0\nmax fnr                      0.986813     0.995146  0\nmax fpr                      0.0354786    1         398\nmax tpr                      0.0316588    1         399\n\nGains/Lift Table: Avg response rate: 41.12 %, avg score: 41.11 %\ngroup    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.011976                    0.978794           2.43204   2.43204            1                0.982327   1                           0.982327            0.0291262       0.0291262                  143.204   143.204            0.0291262\n2        0.0219561                   0.963937           2.43204   2.43204            1                0.969741   1                           0.976606            0.0242718       0.0533981                  143.204   143.204            0.0533981\n3        0.0319361                   0.956702           2.43204   2.43204            1                0.95979    1                           0.971351            0.0242718       0.0776699                  143.204   143.204            0.0776699\n4        0.0419162                   0.949625           1.94563   2.31623            0.8              0.95188    0.952381                    0.966715            0.0194175       0.0970874                  94.5631   131.623            0.0936975\n5        0.0518962                   0.93683            2.43204   2.3385             1                0.94242    0.961538                    0.962043            0.0242718       0.121359                   143.204   133.85             0.117969\n6        0.101796                    0.906174           2.23748   2.28898            0.92             0.919991   0.941176                    0.941429            0.11165         0.23301                    123.748   128.898            0.22284\n7        0.151697                    0.847242           2.33476   2.30404            0.96             0.880731   0.947368                    0.921463            0.116505        0.349515                   133.476   130.404            0.335955\n8        0.201597                    0.775597           2.23748   2.28756            0.92             0.807483   0.940594                    0.89325             0.11165         0.461165                   123.748   128.756            0.440826\n9        0.301397                    0.608629           1.50786   2.02938            0.62             0.676915   0.834437                    0.821616            0.150485        0.61165                    50.7864   102.938            0.526905\n10       0.401198                    0.463678           1.26466   1.83915            0.52             0.5236     0.756219                    0.747483            0.126214        0.737864                   26.466    83.9154            0.571762\n11       0.500998                    0.291777           0.63233   1.59875            0.26             0.373261   0.657371                    0.672936            0.0631068       0.800971                   -36.767   59.8751            0.509445\n12       0.600798                    0.212819           0.486408  1.41398            0.2              0.242084   0.581395                    0.601366            0.0485437       0.849515                   -51.3592  41.3976            0.422396\n13       0.700599                    0.162015           0.486408  1.28184            0.2              0.185801   0.527066                    0.542169            0.0485437       0.898058                   -51.3592  28.1844            0.335346\n14       0.800399                    0.124709           0.486408  1.18266            0.2              0.142232   0.486284                    0.492301            0.0485437       0.946602                   -51.3592  18.2662            0.248297\n15       0.9002                      0.0859383          0.340485  1.08929            0.14             0.105718   0.447894                    0.449443            0.0339806       0.980583                   -65.9515  8.92946            0.136515\n16       1                           0.0316588          0.194563  1                  0.08             0.0655716  0.411178                    0.411132            0.0194175       1                          -80.5437  0                  0\n\nCross-Validation Metrics Summary: \n                      mean        sd           cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n--------------------  ----------  -----------  ------------  ------------  ------------  ------------  ------------\naccuracy              0.8231532   0.048069436  0.78          0.82954544    0.8712871     0.7676768     0.86725664\nauc                   0.8456506   0.037110217  0.85291666    0.7946623     0.8629908     0.8256198     0.8920635\nerr                   0.1768468   0.048069436  0.22          0.17045455    0.12871288    0.23232323    0.13274336\nerr_count             17.6        4.560702     22.0          15.0          13.0          23.0          15.0\nf0point5              0.796463    0.069022     0.7083333     0.8333333     0.8426966     0.73660713    0.8613445\nf1                    0.7802499   0.049913913  0.75555557    0.7368421     0.82191783    0.74157304    0.8453608\nf2                    0.7697212   0.068472974  0.8095238     0.6603774     0.80213904    0.74660635    0.8299595\nlift_top_group        2.451226    0.1876977    2.5           2.5882354     2.6578948     2.25          2.26\nlogloss               0.4596301   0.040655814  0.449594      0.50526917    0.42467046    0.49934316    0.41927382\nmax_per_class_error   0.25790918  0.07736355   0.26666668    0.38235295    0.21052632    0.25          0.18\n---                   ---         ---          ---           ---           ---           ---           ---\nmean_per_class_error  0.1869368   0.042996366  0.20833333    0.209695      0.1449457     0.23409091    0.13761905\nmse                   0.14690518  0.017054295  0.14610153    0.16552782    0.13290341    0.16249818    0.12749498\nnull_deviance         135.92384   13.548579    134.683       117.73908     134.56567     136.71875     155.9127\npr_auc                0.8313398   0.02789983   0.8331954     0.78921276    0.8502845     0.8226492     0.86135715\nprecision             0.811172    0.09937007   0.68          0.9130435     0.85714287    0.73333335    0.87234044\nr2                    0.39036426  0.07190635   0.3912436     0.301826      0.43368936    0.34188238    0.4831799\nrecall                0.76542413  0.090534456  0.85          0.61764705    0.7894737     0.75          0.82\nresidual_deviance     91.651085   5.1623745    89.9188       88.927376     85.78343      98.86994      94.75588\nrmse                  0.38276353  0.022284197  0.3822323     0.4068511     0.3645592     0.40311062    0.3570644\nspecificity           0.8607023   0.09802546   0.73333335    0.962963      0.9206349     0.7818182     0.9047619\n[22 rows x 8 columns]\n\n\n[tips]\nUse `model.explain()` to inspect the model.\n--\nUse `h2o.display.toggle_user_tips()` to switch on/off this section.",
      "text/html": "<pre style='margin: 1em 0 1em 0;'>Model Details\n=============\nH2OStackedEnsembleEstimator : Stacked Ensemble\nModel Key: StackedEnsemble_BestOfFamily_3_AutoML_1_20230224_110317\n</pre>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-2.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-2 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-2 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-2 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-2 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-2 .h2o-table th,\n#h2o-table-2 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-2 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-2\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Model Summary for Stacked Ensemble: </caption>\n    <thead><tr><th>key</th>\n<th>value</th></tr></thead>\n    <tbody><tr><td>Stacking strategy</td>\n<td>cross_validation</td></tr>\n<tr><td>Number of base models (used / total)</td>\n<td>6/6</td></tr>\n<tr><td># GBM base models (used / total)</td>\n<td>1/1</td></tr>\n<tr><td># XGBoost base models (used / total)</td>\n<td>1/1</td></tr>\n<tr><td># DRF base models (used / total)</td>\n<td>2/2</td></tr>\n<tr><td># GLM base models (used / total)</td>\n<td>1/1</td></tr>\n<tr><td># DeepLearning base models (used / total)</td>\n<td>1/1</td></tr>\n<tr><td>Metalearner algorithm</td>\n<td>GLM</td></tr>\n<tr><td>Metalearner fold assignment scheme</td>\n<td>Random</td></tr>\n<tr><td>Metalearner nfolds</td>\n<td>5</td></tr>\n<tr><td>Metalearner fold_column</td>\n<td>None</td></tr>\n<tr><td>Custom metalearner hyperparameters</td>\n<td>None</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.090302569991916\nRMSE: 0.3005038601947003\nLogLoss: 0.30788977805421525\nAUC: 0.9528632548955077\nAUCPR: 0.9442040059021868\nGini: 0.9057265097910154\nNull degrees of freedom: 500\nResidual degrees of freedom: 494\nNull deviance: 678.6388715052319\nResidual deviance: 308.5055576103236\nAIC: 322.5055576103236</pre>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-3.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-3 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-3 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-3 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-3 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-3 .h2o-table th,\n#h2o-table-3 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-3 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-3\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.48698351606560014</caption>\n    <thead><tr><th></th>\n<th>0</th>\n<th>1</th>\n<th>Error</th>\n<th>Rate</th></tr></thead>\n    <tbody><tr><td>0</td>\n<td>282.0</td>\n<td>13.0</td>\n<td>0.0441</td>\n<td> (13.0/295.0)</td></tr>\n<tr><td>1</td>\n<td>39.0</td>\n<td>167.0</td>\n<td>0.1893</td>\n<td> (39.0/206.0)</td></tr>\n<tr><td>Total</td>\n<td>321.0</td>\n<td>180.0</td>\n<td>0.1038</td>\n<td> (52.0/501.0)</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-4.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-4 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-4 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-4 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-4 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-4 .h2o-table th,\n#h2o-table-4 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-4 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-4\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n    <thead><tr><th>metric</th>\n<th>threshold</th>\n<th>value</th>\n<th>idx</th></tr></thead>\n    <tbody><tr><td>max f1</td>\n<td>0.4869835</td>\n<td>0.8652850</td>\n<td>155.0</td></tr>\n<tr><td>max f2</td>\n<td>0.2601193</td>\n<td>0.8998145</td>\n<td>220.0</td></tr>\n<tr><td>max f0point5</td>\n<td>0.5490708</td>\n<td>0.9111617</td>\n<td>145.0</td></tr>\n<tr><td>max accuracy</td>\n<td>0.4869835</td>\n<td>0.8962076</td>\n<td>155.0</td></tr>\n<tr><td>max precision</td>\n<td>0.9846315</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max recall</td>\n<td>0.1130783</td>\n<td>1.0</td>\n<td>320.0</td></tr>\n<tr><td>max specificity</td>\n<td>0.9846315</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max absolute_mcc</td>\n<td>0.4869835</td>\n<td>0.7861970</td>\n<td>155.0</td></tr>\n<tr><td>max min_per_class_accuracy</td>\n<td>0.3415439</td>\n<td>0.8640777</td>\n<td>190.0</td></tr>\n<tr><td>max mean_per_class_accuracy</td>\n<td>0.4869835</td>\n<td>0.8833059</td>\n<td>155.0</td></tr>\n<tr><td>max tns</td>\n<td>0.9846315</td>\n<td>295.0</td>\n<td>0.0</td></tr>\n<tr><td>max fns</td>\n<td>0.9846315</td>\n<td>205.0</td>\n<td>0.0</td></tr>\n<tr><td>max fps</td>\n<td>0.0271721</td>\n<td>295.0</td>\n<td>399.0</td></tr>\n<tr><td>max tps</td>\n<td>0.1130783</td>\n<td>206.0</td>\n<td>320.0</td></tr>\n<tr><td>max tnr</td>\n<td>0.9846315</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max fnr</td>\n<td>0.9846315</td>\n<td>0.9951456</td>\n<td>0.0</td></tr>\n<tr><td>max fpr</td>\n<td>0.0271721</td>\n<td>1.0</td>\n<td>399.0</td></tr>\n<tr><td>max tpr</td>\n<td>0.1130783</td>\n<td>1.0</td>\n<td>320.0</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-5.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-5 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-5 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-5 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-5 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-5 .h2o-table th,\n#h2o-table-5 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-5 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-5\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Gains/Lift Table: Avg response rate: 41.12 %, avg score: 40.55 %</caption>\n    <thead><tr><th>group</th>\n<th>cumulative_data_fraction</th>\n<th>lower_threshold</th>\n<th>lift</th>\n<th>cumulative_lift</th>\n<th>response_rate</th>\n<th>score</th>\n<th>cumulative_response_rate</th>\n<th>cumulative_score</th>\n<th>capture_rate</th>\n<th>cumulative_capture_rate</th>\n<th>gain</th>\n<th>cumulative_gain</th>\n<th>kolmogorov_smirnov</th></tr></thead>\n    <tbody><tr><td>1</td>\n<td>0.0119760</td>\n<td>0.9787792</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9806054</td>\n<td>1.0</td>\n<td>0.9806054</td>\n<td>0.0291262</td>\n<td>0.0291262</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0291262</td></tr>\n<tr><td>2</td>\n<td>0.0219561</td>\n<td>0.9725229</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9760010</td>\n<td>1.0</td>\n<td>0.9785125</td>\n<td>0.0242718</td>\n<td>0.0533981</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0533981</td></tr>\n<tr><td>3</td>\n<td>0.0319361</td>\n<td>0.9699859</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9708564</td>\n<td>1.0</td>\n<td>0.9761200</td>\n<td>0.0242718</td>\n<td>0.0776699</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0776699</td></tr>\n<tr><td>4</td>\n<td>0.0419162</td>\n<td>0.9682736</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9688142</td>\n<td>1.0</td>\n<td>0.9743805</td>\n<td>0.0242718</td>\n<td>0.1019417</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.1019417</td></tr>\n<tr><td>5</td>\n<td>0.0518962</td>\n<td>0.9577170</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9635359</td>\n<td>1.0</td>\n<td>0.9722950</td>\n<td>0.0242718</td>\n<td>0.1262136</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.1262136</td></tr>\n<tr><td>6</td>\n<td>0.1017964</td>\n<td>0.9211650</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9447383</td>\n<td>1.0</td>\n<td>0.9587868</td>\n<td>0.1213592</td>\n<td>0.2475728</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.2475728</td></tr>\n<tr><td>7</td>\n<td>0.1516966</td>\n<td>0.8827008</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9030155</td>\n<td>1.0</td>\n<td>0.9404410</td>\n<td>0.1213592</td>\n<td>0.3689320</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.3689320</td></tr>\n<tr><td>8</td>\n<td>0.2015968</td>\n<td>0.8208444</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.8596583</td>\n<td>1.0</td>\n<td>0.9204453</td>\n<td>0.1213592</td>\n<td>0.4902913</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.4902913</td></tr>\n<tr><td>9</td>\n<td>0.3013972</td>\n<td>0.6492357</td>\n<td>2.1401942</td>\n<td>2.3354015</td>\n<td>0.88</td>\n<td>0.7207865</td>\n<td>0.9602649</td>\n<td>0.8543331</td>\n<td>0.2135922</td>\n<td>0.7038835</td>\n<td>114.0194175</td>\n<td>133.5401530</td>\n<td>0.6835445</td></tr>\n<tr><td>10</td>\n<td>0.4011976</td>\n<td>0.4075139</td>\n<td>1.2160194</td>\n<td>2.0569483</td>\n<td>0.5</td>\n<td>0.5133141</td>\n<td>0.8457711</td>\n<td>0.7695025</td>\n<td>0.1213592</td>\n<td>0.8252427</td>\n<td>21.6019417</td>\n<td>105.6948268</td>\n<td>0.7201580</td></tr>\n<tr><td>11</td>\n<td>0.5009980</td>\n<td>0.2609198</td>\n<td>1.0700971</td>\n<td>1.8603644</td>\n<td>0.44</td>\n<td>0.3220798</td>\n<td>0.7649402</td>\n<td>0.6803745</td>\n<td>0.1067961</td>\n<td>0.9320388</td>\n<td>7.0097087</td>\n<td>86.0364368</td>\n<td>0.7320388</td></tr>\n<tr><td>12</td>\n<td>0.6007984</td>\n<td>0.1818721</td>\n<td>0.3404854</td>\n<td>1.6078928</td>\n<td>0.14</td>\n<td>0.2198483</td>\n<td>0.6611296</td>\n<td>0.6038751</td>\n<td>0.0339806</td>\n<td>0.9660194</td>\n<td>-65.9514563</td>\n<td>60.7892785</td>\n<td>0.6202567</td></tr>\n<tr><td>13</td>\n<td>0.7005988</td>\n<td>0.1342557</td>\n<td>0.2432039</td>\n<td>1.4134927</td>\n<td>0.1</td>\n<td>0.1600129</td>\n<td>0.5811966</td>\n<td>0.5406469</td>\n<td>0.0242718</td>\n<td>0.9902913</td>\n<td>-75.6796117</td>\n<td>41.3492656</td>\n<td>0.4919862</td></tr>\n<tr><td>14</td>\n<td>0.8003992</td>\n<td>0.1084087</td>\n<td>0.0972816</td>\n<td>1.2493766</td>\n<td>0.04</td>\n<td>0.1209252</td>\n<td>0.5137157</td>\n<td>0.4883125</td>\n<td>0.0097087</td>\n<td>1.0</td>\n<td>-90.2718447</td>\n<td>24.9376559</td>\n<td>0.3389831</td></tr>\n<tr><td>15</td>\n<td>0.9001996</td>\n<td>0.0748283</td>\n<td>0.0</td>\n<td>1.1108647</td>\n<td>0.0</td>\n<td>0.0924047</td>\n<td>0.4567627</td>\n<td>0.4444203</td>\n<td>0.0</td>\n<td>1.0</td>\n<td>-100.0</td>\n<td>11.0864745</td>\n<td>0.1694915</td></tr>\n<tr><td>16</td>\n<td>1.0</td>\n<td>0.0271721</td>\n<td>0.0</td>\n<td>1.0</td>\n<td>0.0</td>\n<td>0.0539791</td>\n<td>0.4111776</td>\n<td>0.4054541</td>\n<td>0.0</td>\n<td>1.0</td>\n<td>-100.0</td>\n<td>0.0</td>\n<td>0.0</td></tr></tbody>\n  </table>\n</div>\n</div></div>\n<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: stackedensemble\n** Reported on validation data. **\n\nMSE: 0.10790103374848198\nRMSE: 0.3284829276362502\nLogLoss: 0.3641385099423577\nAUC: 0.9066458687842692\nAUCPR: 0.8922966766719541\nGini: 0.8132917375685385\nNull degrees of freedom: 210\nResidual degrees of freedom: 204\nNull deviance: 282.39644104064814\nResidual deviance: 153.66645119567494\nAIC: 167.66645119567494</pre>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-6.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-6 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-6 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-6 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-6 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-6 .h2o-table th,\n#h2o-table-6 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-6 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-6\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5038318992954588</caption>\n    <thead><tr><th></th>\n<th>0</th>\n<th>1</th>\n<th>Error</th>\n<th>Rate</th></tr></thead>\n    <tbody><tr><td>0</td>\n<td>117.0</td>\n<td>12.0</td>\n<td>0.093</td>\n<td> (12.0/129.0)</td></tr>\n<tr><td>1</td>\n<td>15.0</td>\n<td>67.0</td>\n<td>0.1829</td>\n<td> (15.0/82.0)</td></tr>\n<tr><td>Total</td>\n<td>132.0</td>\n<td>79.0</td>\n<td>0.128</td>\n<td> (27.0/211.0)</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-7.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-7 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-7 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-7 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-7 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-7 .h2o-table th,\n#h2o-table-7 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-7 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-7\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n    <thead><tr><th>metric</th>\n<th>threshold</th>\n<th>value</th>\n<th>idx</th></tr></thead>\n    <tbody><tr><td>max f1</td>\n<td>0.5038319</td>\n<td>0.8322981</td>\n<td>75.0</td></tr>\n<tr><td>max f2</td>\n<td>0.3102633</td>\n<td>0.8782201</td>\n<td>95.0</td></tr>\n<tr><td>max f0point5</td>\n<td>0.5831289</td>\n<td>0.8615819</td>\n<td>65.0</td></tr>\n<tr><td>max accuracy</td>\n<td>0.5255172</td>\n<td>0.8720379</td>\n<td>71.0</td></tr>\n<tr><td>max precision</td>\n<td>0.9788934</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max recall</td>\n<td>0.0593606</td>\n<td>1.0</td>\n<td>190.0</td></tr>\n<tr><td>max specificity</td>\n<td>0.9788934</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max absolute_mcc</td>\n<td>0.5038319</td>\n<td>0.7292388</td>\n<td>75.0</td></tr>\n<tr><td>max min_per_class_accuracy</td>\n<td>0.4034351</td>\n<td>0.8449612</td>\n<td>86.0</td></tr>\n<tr><td>max mean_per_class_accuracy</td>\n<td>0.3182856</td>\n<td>0.8659482</td>\n<td>92.0</td></tr>\n<tr><td>max tns</td>\n<td>0.9788934</td>\n<td>129.0</td>\n<td>0.0</td></tr>\n<tr><td>max fns</td>\n<td>0.9788934</td>\n<td>81.0</td>\n<td>0.0</td></tr>\n<tr><td>max fps</td>\n<td>0.0269644</td>\n<td>129.0</td>\n<td>204.0</td></tr>\n<tr><td>max tps</td>\n<td>0.0593606</td>\n<td>82.0</td>\n<td>190.0</td></tr>\n<tr><td>max tnr</td>\n<td>0.9788934</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max fnr</td>\n<td>0.9788934</td>\n<td>0.9878049</td>\n<td>0.0</td></tr>\n<tr><td>max fpr</td>\n<td>0.0269644</td>\n<td>1.0</td>\n<td>204.0</td></tr>\n<tr><td>max tpr</td>\n<td>0.0593606</td>\n<td>1.0</td>\n<td>190.0</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-8.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-8 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-8 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-8 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-8 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-8 .h2o-table th,\n#h2o-table-8 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-8 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-8\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Gains/Lift Table: Avg response rate: 38.86 %, avg score: 40.50 %</caption>\n    <thead><tr><th>group</th>\n<th>cumulative_data_fraction</th>\n<th>lower_threshold</th>\n<th>lift</th>\n<th>cumulative_lift</th>\n<th>response_rate</th>\n<th>score</th>\n<th>cumulative_response_rate</th>\n<th>cumulative_score</th>\n<th>capture_rate</th>\n<th>cumulative_capture_rate</th>\n<th>gain</th>\n<th>cumulative_gain</th>\n<th>kolmogorov_smirnov</th></tr></thead>\n    <tbody><tr><td>1</td>\n<td>0.0142180</td>\n<td>0.9729723</td>\n<td>2.5731707</td>\n<td>2.5731707</td>\n<td>1.0</td>\n<td>0.9767317</td>\n<td>1.0</td>\n<td>0.9767317</td>\n<td>0.0365854</td>\n<td>0.0365854</td>\n<td>157.3170732</td>\n<td>157.3170732</td>\n<td>0.0365854</td></tr>\n<tr><td>2</td>\n<td>0.0236967</td>\n<td>0.9583031</td>\n<td>2.5731707</td>\n<td>2.5731707</td>\n<td>1.0</td>\n<td>0.9631331</td>\n<td>1.0</td>\n<td>0.9712923</td>\n<td>0.0243902</td>\n<td>0.0609756</td>\n<td>157.3170732</td>\n<td>157.3170732</td>\n<td>0.0609756</td></tr>\n<tr><td>3</td>\n<td>0.0331754</td>\n<td>0.9562840</td>\n<td>2.5731707</td>\n<td>2.5731707</td>\n<td>1.0</td>\n<td>0.9565058</td>\n<td>1.0</td>\n<td>0.9670676</td>\n<td>0.0243902</td>\n<td>0.0853659</td>\n<td>157.3170732</td>\n<td>157.3170732</td>\n<td>0.0853659</td></tr>\n<tr><td>4</td>\n<td>0.0426540</td>\n<td>0.9540521</td>\n<td>2.5731707</td>\n<td>2.5731707</td>\n<td>1.0</td>\n<td>0.9557039</td>\n<td>1.0</td>\n<td>0.9645423</td>\n<td>0.0243902</td>\n<td>0.1097561</td>\n<td>157.3170732</td>\n<td>157.3170732</td>\n<td>0.1097561</td></tr>\n<tr><td>5</td>\n<td>0.0521327</td>\n<td>0.9501502</td>\n<td>2.5731707</td>\n<td>2.5731707</td>\n<td>1.0</td>\n<td>0.9513399</td>\n<td>1.0</td>\n<td>0.9621419</td>\n<td>0.0243902</td>\n<td>0.1341463</td>\n<td>157.3170732</td>\n<td>157.3170732</td>\n<td>0.1341463</td></tr>\n<tr><td>6</td>\n<td>0.1042654</td>\n<td>0.9263479</td>\n<td>2.3392461</td>\n<td>2.4562084</td>\n<td>0.9090909</td>\n<td>0.9391181</td>\n<td>0.9545455</td>\n<td>0.9506300</td>\n<td>0.1219512</td>\n<td>0.2560976</td>\n<td>133.9246120</td>\n<td>145.6208426</td>\n<td>0.2483456</td></tr>\n<tr><td>7</td>\n<td>0.1516588</td>\n<td>0.8954393</td>\n<td>2.5731707</td>\n<td>2.4927591</td>\n<td>1.0</td>\n<td>0.9123990</td>\n<td>0.96875</td>\n<td>0.9386828</td>\n<td>0.1219512</td>\n<td>0.3780488</td>\n<td>157.3170732</td>\n<td>149.2759146</td>\n<td>0.3702968</td></tr>\n<tr><td>8</td>\n<td>0.2037915</td>\n<td>0.7389430</td>\n<td>2.3392461</td>\n<td>2.4534884</td>\n<td>0.9090909</td>\n<td>0.8539613</td>\n<td>0.9534884</td>\n<td>0.9170099</td>\n<td>0.1219512</td>\n<td>0.5</td>\n<td>133.9246120</td>\n<td>145.3488372</td>\n<td>0.4844961</td></tr>\n<tr><td>9</td>\n<td>0.3033175</td>\n<td>0.6173731</td>\n<td>1.9605110</td>\n<td>2.2917302</td>\n<td>0.7619048</td>\n<td>0.6897821</td>\n<td>0.890625</td>\n<td>0.8424508</td>\n<td>0.1951220</td>\n<td>0.6951220</td>\n<td>96.0511034</td>\n<td>129.1730183</td>\n<td>0.6408584</td></tr>\n<tr><td>10</td>\n<td>0.4028436</td>\n<td>0.4281485</td>\n<td>1.3478513</td>\n<td>2.0585366</td>\n<td>0.5238095</td>\n<td>0.5262224</td>\n<td>0.8</td>\n<td>0.7643237</td>\n<td>0.1341463</td>\n<td>0.8292683</td>\n<td>34.7851336</td>\n<td>105.8536585</td>\n<td>0.6974853</td></tr>\n<tr><td>11</td>\n<td>0.5023697</td>\n<td>0.2558777</td>\n<td>0.8577236</td>\n<td>1.8206397</td>\n<td>0.3333333</td>\n<td>0.3357459</td>\n<td>0.7075472</td>\n<td>0.6794168</td>\n<td>0.0853659</td>\n<td>0.9146341</td>\n<td>-14.2276423</td>\n<td>82.0639669</td>\n<td>0.6743241</td></tr>\n<tr><td>12</td>\n<td>0.6018957</td>\n<td>0.1868997</td>\n<td>0.1225319</td>\n<td>1.5398502</td>\n<td>0.0476190</td>\n<td>0.2207721</td>\n<td>0.5984252</td>\n<td>0.6035779</td>\n<td>0.0121951</td>\n<td>0.9268293</td>\n<td>-87.7468060</td>\n<td>53.9850202</td>\n<td>0.5314804</td></tr>\n<tr><td>13</td>\n<td>0.7014218</td>\n<td>0.1320844</td>\n<td>0.1225319</td>\n<td>1.3387442</td>\n<td>0.0476190</td>\n<td>0.1626075</td>\n<td>0.5202703</td>\n<td>0.5410078</td>\n<td>0.0121951</td>\n<td>0.9390244</td>\n<td>-87.7468060</td>\n<td>33.8744232</td>\n<td>0.3886368</td></tr>\n<tr><td>14</td>\n<td>0.8009479</td>\n<td>0.1004387</td>\n<td>0.1225319</td>\n<td>1.1876173</td>\n<td>0.0476190</td>\n<td>0.1153699</td>\n<td>0.4615385</td>\n<td>0.4881179</td>\n<td>0.0121951</td>\n<td>0.9512195</td>\n<td>-87.7468060</td>\n<td>18.7617261</td>\n<td>0.2457932</td></tr>\n<tr><td>15</td>\n<td>0.9004739</td>\n<td>0.0727439</td>\n<td>0.2450639</td>\n<td>1.0834403</td>\n<td>0.0952381</td>\n<td>0.0858364</td>\n<td>0.4210526</td>\n<td>0.4436552</td>\n<td>0.0243902</td>\n<td>0.9756098</td>\n<td>-75.4936121</td>\n<td>8.3440308</td>\n<td>0.1228966</td></tr>\n<tr><td>16</td>\n<td>1.0</td>\n<td>0.0269644</td>\n<td>0.2450639</td>\n<td>1.0</td>\n<td>0.0952381</td>\n<td>0.0548286</td>\n<td>0.3886256</td>\n<td>0.4049568</td>\n<td>0.0243902</td>\n<td>1.0</td>\n<td>-75.4936121</td>\n<td>0.0</td>\n<td>0.0</td></tr></tbody>\n  </table>\n</div>\n</div></div>\n<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: stackedensemble\n** Reported on cross-validation data. **\n\nMSE: 0.14648277687501338\nRMSE: 0.38273068452243725\nLogLoss: 0.4593560459236398\nAUC: 0.8461329603422741\nAUCPR: 0.8292932326677792\nGini: 0.6922659206845483\nNull degrees of freedom: 500\nResidual degrees of freedom: 494\nNull deviance: 679.6192027459856\nResidual deviance: 460.2747580154872\nAIC: 474.2747580154872</pre>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-9.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-9 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-9 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-9 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-9 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-9 .h2o-table th,\n#h2o-table-9 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-9 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-9\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.42980900650465503</caption>\n    <thead><tr><th></th>\n<th>0</th>\n<th>1</th>\n<th>Error</th>\n<th>Rate</th></tr></thead>\n    <tbody><tr><td>0</td>\n<td>241.0</td>\n<td>54.0</td>\n<td>0.1831</td>\n<td> (54.0/295.0)</td></tr>\n<tr><td>1</td>\n<td>46.0</td>\n<td>160.0</td>\n<td>0.2233</td>\n<td> (46.0/206.0)</td></tr>\n<tr><td>Total</td>\n<td>287.0</td>\n<td>214.0</td>\n<td>0.1996</td>\n<td> (100.0/501.0)</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-10.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-10 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-10 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-10 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-10 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-10 .h2o-table th,\n#h2o-table-10 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-10 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-10\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n    <thead><tr><th>metric</th>\n<th>threshold</th>\n<th>value</th>\n<th>idx</th></tr></thead>\n    <tbody><tr><td>max f1</td>\n<td>0.4298090</td>\n<td>0.7619048</td>\n<td>189.0</td></tr>\n<tr><td>max f2</td>\n<td>0.1390808</td>\n<td>0.8098007</td>\n<td>322.0</td></tr>\n<tr><td>max f0point5</td>\n<td>0.7528128</td>\n<td>0.8076923</td>\n<td>91.0</td></tr>\n<tr><td>max accuracy</td>\n<td>0.5150953</td>\n<td>0.8083832</td>\n<td>156.0</td></tr>\n<tr><td>max precision</td>\n<td>0.9868134</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max recall</td>\n<td>0.0316588</td>\n<td>1.0</td>\n<td>399.0</td></tr>\n<tr><td>max specificity</td>\n<td>0.9868134</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max absolute_mcc</td>\n<td>0.5150953</td>\n<td>0.6001909</td>\n<td>156.0</td></tr>\n<tr><td>max min_per_class_accuracy</td>\n<td>0.3793733</td>\n<td>0.7864078</td>\n<td>199.0</td></tr>\n<tr><td>max mean_per_class_accuracy</td>\n<td>0.4298090</td>\n<td>0.7968241</td>\n<td>189.0</td></tr>\n<tr><td>max tns</td>\n<td>0.9868134</td>\n<td>295.0</td>\n<td>0.0</td></tr>\n<tr><td>max fns</td>\n<td>0.9868134</td>\n<td>205.0</td>\n<td>0.0</td></tr>\n<tr><td>max fps</td>\n<td>0.0354786</td>\n<td>295.0</td>\n<td>398.0</td></tr>\n<tr><td>max tps</td>\n<td>0.0316588</td>\n<td>206.0</td>\n<td>399.0</td></tr>\n<tr><td>max tnr</td>\n<td>0.9868134</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max fnr</td>\n<td>0.9868134</td>\n<td>0.9951456</td>\n<td>0.0</td></tr>\n<tr><td>max fpr</td>\n<td>0.0354786</td>\n<td>1.0</td>\n<td>398.0</td></tr>\n<tr><td>max tpr</td>\n<td>0.0316588</td>\n<td>1.0</td>\n<td>399.0</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-11.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-11 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-11 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-11 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-11 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-11 .h2o-table th,\n#h2o-table-11 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-11 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-11\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Gains/Lift Table: Avg response rate: 41.12 %, avg score: 41.11 %</caption>\n    <thead><tr><th>group</th>\n<th>cumulative_data_fraction</th>\n<th>lower_threshold</th>\n<th>lift</th>\n<th>cumulative_lift</th>\n<th>response_rate</th>\n<th>score</th>\n<th>cumulative_response_rate</th>\n<th>cumulative_score</th>\n<th>capture_rate</th>\n<th>cumulative_capture_rate</th>\n<th>gain</th>\n<th>cumulative_gain</th>\n<th>kolmogorov_smirnov</th></tr></thead>\n    <tbody><tr><td>1</td>\n<td>0.0119760</td>\n<td>0.9787940</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9823272</td>\n<td>1.0</td>\n<td>0.9823272</td>\n<td>0.0291262</td>\n<td>0.0291262</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0291262</td></tr>\n<tr><td>2</td>\n<td>0.0219561</td>\n<td>0.9639371</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9697411</td>\n<td>1.0</td>\n<td>0.9766062</td>\n<td>0.0242718</td>\n<td>0.0533981</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0533981</td></tr>\n<tr><td>3</td>\n<td>0.0319361</td>\n<td>0.9567024</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9597904</td>\n<td>1.0</td>\n<td>0.9713513</td>\n<td>0.0242718</td>\n<td>0.0776699</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0776699</td></tr>\n<tr><td>4</td>\n<td>0.0419162</td>\n<td>0.9496254</td>\n<td>1.9456311</td>\n<td>2.3162275</td>\n<td>0.8</td>\n<td>0.9518798</td>\n<td>0.9523810</td>\n<td>0.9667152</td>\n<td>0.0194175</td>\n<td>0.0970874</td>\n<td>94.5631068</td>\n<td>131.6227462</td>\n<td>0.0936975</td></tr>\n<tr><td>5</td>\n<td>0.0518962</td>\n<td>0.9368296</td>\n<td>2.4320388</td>\n<td>2.3384989</td>\n<td>1.0</td>\n<td>0.9424199</td>\n<td>0.9615385</td>\n<td>0.9620430</td>\n<td>0.0242718</td>\n<td>0.1213592</td>\n<td>143.2038835</td>\n<td>133.8498880</td>\n<td>0.1179694</td></tr>\n<tr><td>6</td>\n<td>0.1017964</td>\n<td>0.9061738</td>\n<td>2.2374757</td>\n<td>2.2889777</td>\n<td>0.92</td>\n<td>0.9199909</td>\n<td>0.9411765</td>\n<td>0.9414293</td>\n<td>0.1116505</td>\n<td>0.2330097</td>\n<td>123.7475728</td>\n<td>128.8977727</td>\n<td>0.2228402</td></tr>\n<tr><td>7</td>\n<td>0.1516966</td>\n<td>0.8472418</td>\n<td>2.3347573</td>\n<td>2.3040368</td>\n<td>0.96</td>\n<td>0.8807309</td>\n<td>0.9473684</td>\n<td>0.9214627</td>\n<td>0.1165049</td>\n<td>0.3495146</td>\n<td>133.4757282</td>\n<td>130.4036791</td>\n<td>0.3359552</td></tr>\n<tr><td>8</td>\n<td>0.2015968</td>\n<td>0.7755974</td>\n<td>2.2374757</td>\n<td>2.2875613</td>\n<td>0.92</td>\n<td>0.8074830</td>\n<td>0.9405941</td>\n<td>0.8932499</td>\n<td>0.1116505</td>\n<td>0.4611650</td>\n<td>123.7475728</td>\n<td>128.7561280</td>\n<td>0.4408261</td></tr>\n<tr><td>9</td>\n<td>0.3013972</td>\n<td>0.6086286</td>\n<td>1.5078641</td>\n<td>2.0293834</td>\n<td>0.62</td>\n<td>0.6769152</td>\n<td>0.8344371</td>\n<td>0.8216159</td>\n<td>0.1504854</td>\n<td>0.6116505</td>\n<td>50.7864078</td>\n<td>102.9383399</td>\n<td>0.5269047</td></tr>\n<tr><td>10</td>\n<td>0.4011976</td>\n<td>0.4636780</td>\n<td>1.2646602</td>\n<td>1.8391537</td>\n<td>0.52</td>\n<td>0.5236000</td>\n<td>0.7562189</td>\n<td>0.7474826</td>\n<td>0.1262136</td>\n<td>0.7378641</td>\n<td>26.4660194</td>\n<td>83.9153746</td>\n<td>0.5717624</td></tr>\n<tr><td>11</td>\n<td>0.5009980</td>\n<td>0.2917765</td>\n<td>0.6323301</td>\n<td>1.5987506</td>\n<td>0.26</td>\n<td>0.3732606</td>\n<td>0.6573705</td>\n<td>0.6729364</td>\n<td>0.0631068</td>\n<td>0.8009709</td>\n<td>-36.7669903</td>\n<td>59.8750629</td>\n<td>0.5094455</td></tr>\n<tr><td>12</td>\n<td>0.6007984</td>\n<td>0.2128191</td>\n<td>0.4864078</td>\n<td>1.4139761</td>\n<td>0.2</td>\n<td>0.2420837</td>\n<td>0.5813953</td>\n<td>0.6013661</td>\n<td>0.0485437</td>\n<td>0.8495146</td>\n<td>-51.3592233</td>\n<td>41.3976067</td>\n<td>0.4223959</td></tr>\n<tr><td>13</td>\n<td>0.7005988</td>\n<td>0.1620148</td>\n<td>0.4864078</td>\n<td>1.2818438</td>\n<td>0.2</td>\n<td>0.1858014</td>\n<td>0.5270655</td>\n<td>0.5421689</td>\n<td>0.0485437</td>\n<td>0.8980583</td>\n<td>-51.3592233</td>\n<td>28.1843830</td>\n<td>0.3353464</td></tr>\n<tr><td>14</td>\n<td>0.8003992</td>\n<td>0.1247093</td>\n<td>0.4864078</td>\n<td>1.1826623</td>\n<td>0.2</td>\n<td>0.1422316</td>\n<td>0.4862843</td>\n<td>0.4923014</td>\n<td>0.0485437</td>\n<td>0.9466019</td>\n<td>-51.3592233</td>\n<td>18.2662276</td>\n<td>0.2482969</td></tr>\n<tr><td>15</td>\n<td>0.9001996</td>\n<td>0.0859383</td>\n<td>0.3404854</td>\n<td>1.0892946</td>\n<td>0.14</td>\n<td>0.1057175</td>\n<td>0.4478936</td>\n<td>0.4494429</td>\n<td>0.0339806</td>\n<td>0.9805825</td>\n<td>-65.9514563</td>\n<td>8.9294556</td>\n<td>0.1365147</td></tr>\n<tr><td>16</td>\n<td>1.0</td>\n<td>0.0316588</td>\n<td>0.1945631</td>\n<td>1.0</td>\n<td>0.08</td>\n<td>0.0655716</td>\n<td>0.4111776</td>\n<td>0.4111324</td>\n<td>0.0194175</td>\n<td>1.0</td>\n<td>-80.5436893</td>\n<td>0.0</td>\n<td>0.0</td></tr></tbody>\n  </table>\n</div>\n</div></div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-12.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-12 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-12 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-12 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-12 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-12 .h2o-table th,\n#h2o-table-12 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-12 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-12\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Cross-Validation Metrics Summary: </caption>\n    <thead><tr><th></th>\n<th>mean</th>\n<th>sd</th>\n<th>cv_1_valid</th>\n<th>cv_2_valid</th>\n<th>cv_3_valid</th>\n<th>cv_4_valid</th>\n<th>cv_5_valid</th></tr></thead>\n    <tbody><tr><td>accuracy</td>\n<td>0.8231532</td>\n<td>0.0480694</td>\n<td>0.78</td>\n<td>0.8295454</td>\n<td>0.8712871</td>\n<td>0.7676768</td>\n<td>0.8672566</td></tr>\n<tr><td>auc</td>\n<td>0.8456506</td>\n<td>0.0371102</td>\n<td>0.8529167</td>\n<td>0.7946623</td>\n<td>0.8629908</td>\n<td>0.8256198</td>\n<td>0.8920635</td></tr>\n<tr><td>err</td>\n<td>0.1768468</td>\n<td>0.0480694</td>\n<td>0.22</td>\n<td>0.1704546</td>\n<td>0.1287129</td>\n<td>0.2323232</td>\n<td>0.1327434</td></tr>\n<tr><td>err_count</td>\n<td>17.6</td>\n<td>4.560702</td>\n<td>22.0</td>\n<td>15.0</td>\n<td>13.0</td>\n<td>23.0</td>\n<td>15.0</td></tr>\n<tr><td>f0point5</td>\n<td>0.796463</td>\n<td>0.069022</td>\n<td>0.7083333</td>\n<td>0.8333333</td>\n<td>0.8426966</td>\n<td>0.7366071</td>\n<td>0.8613445</td></tr>\n<tr><td>f1</td>\n<td>0.7802499</td>\n<td>0.0499139</td>\n<td>0.7555556</td>\n<td>0.7368421</td>\n<td>0.8219178</td>\n<td>0.7415730</td>\n<td>0.8453608</td></tr>\n<tr><td>f2</td>\n<td>0.7697212</td>\n<td>0.0684730</td>\n<td>0.8095238</td>\n<td>0.6603774</td>\n<td>0.8021390</td>\n<td>0.7466063</td>\n<td>0.8299595</td></tr>\n<tr><td>lift_top_group</td>\n<td>2.451226</td>\n<td>0.1876977</td>\n<td>2.5</td>\n<td>2.5882354</td>\n<td>2.6578948</td>\n<td>2.25</td>\n<td>2.26</td></tr>\n<tr><td>logloss</td>\n<td>0.4596301</td>\n<td>0.0406558</td>\n<td>0.449594</td>\n<td>0.5052692</td>\n<td>0.4246705</td>\n<td>0.4993432</td>\n<td>0.4192738</td></tr>\n<tr><td>max_per_class_error</td>\n<td>0.2579092</td>\n<td>0.0773636</td>\n<td>0.2666667</td>\n<td>0.3823530</td>\n<td>0.2105263</td>\n<td>0.25</td>\n<td>0.18</td></tr>\n<tr><td>---</td>\n<td>---</td>\n<td>---</td>\n<td>---</td>\n<td>---</td>\n<td>---</td>\n<td>---</td>\n<td>---</td></tr>\n<tr><td>mean_per_class_error</td>\n<td>0.1869368</td>\n<td>0.0429964</td>\n<td>0.2083333</td>\n<td>0.209695</td>\n<td>0.1449457</td>\n<td>0.2340909</td>\n<td>0.1376190</td></tr>\n<tr><td>mse</td>\n<td>0.1469052</td>\n<td>0.0170543</td>\n<td>0.1461015</td>\n<td>0.1655278</td>\n<td>0.1329034</td>\n<td>0.1624982</td>\n<td>0.1274950</td></tr>\n<tr><td>null_deviance</td>\n<td>135.92384</td>\n<td>13.548579</td>\n<td>134.683</td>\n<td>117.73908</td>\n<td>134.56567</td>\n<td>136.71875</td>\n<td>155.9127</td></tr>\n<tr><td>pr_auc</td>\n<td>0.8313398</td>\n<td>0.0278998</td>\n<td>0.8331954</td>\n<td>0.7892128</td>\n<td>0.8502845</td>\n<td>0.8226492</td>\n<td>0.8613572</td></tr>\n<tr><td>precision</td>\n<td>0.811172</td>\n<td>0.0993701</td>\n<td>0.68</td>\n<td>0.9130435</td>\n<td>0.8571429</td>\n<td>0.7333333</td>\n<td>0.8723404</td></tr>\n<tr><td>r2</td>\n<td>0.3903643</td>\n<td>0.0719063</td>\n<td>0.3912436</td>\n<td>0.301826</td>\n<td>0.4336894</td>\n<td>0.3418824</td>\n<td>0.4831799</td></tr>\n<tr><td>recall</td>\n<td>0.7654241</td>\n<td>0.0905345</td>\n<td>0.85</td>\n<td>0.6176470</td>\n<td>0.7894737</td>\n<td>0.75</td>\n<td>0.82</td></tr>\n<tr><td>residual_deviance</td>\n<td>91.651085</td>\n<td>5.1623745</td>\n<td>89.9188</td>\n<td>88.927376</td>\n<td>85.78343</td>\n<td>98.86994</td>\n<td>94.75588</td></tr>\n<tr><td>rmse</td>\n<td>0.3827635</td>\n<td>0.0222842</td>\n<td>0.3822323</td>\n<td>0.4068511</td>\n<td>0.3645592</td>\n<td>0.4031106</td>\n<td>0.3570644</td></tr>\n<tr><td>specificity</td>\n<td>0.8607023</td>\n<td>0.0980255</td>\n<td>0.7333333</td>\n<td>0.962963</td>\n<td>0.9206349</td>\n<td>0.7818182</td>\n<td>0.9047619</td></tr></tbody>\n  </table>\n</div>\n<pre style='font-size: smaller; margin-bottom: 1em;'>[22 rows x 8 columns]</pre></div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n\n[tips]\nUse `model.explain()` to inspect the model.\n--\nUse `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml = H2OAutoML(max_runtime_secs = 120)\n",
    "aml.train(x = x, y = y, training_frame = train, validation_frame = test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "model_id                                                                       auc    logloss     aucpr    mean_per_class_error      rmse       mse\n------------------------------------------------------------------------  --------  ---------  --------  ----------------------  --------  --------\nStackedEnsemble_BestOfFamily_3_AutoML_1_20230224_110317                   0.846133   0.459356  0.829293                0.203176  0.382731  0.146483\nGBM_grid_1_AutoML_1_20230224_110317_model_12                              0.845606   0.465041  0.827992                0.220166  0.386294  0.149223\nGBM_grid_1_AutoML_1_20230224_110317_model_9                               0.844734   0.458622  0.831179                0.208532  0.382822  0.146552\nStackedEnsemble_BestOfFamily_4_AutoML_1_20230224_110317                   0.844405   0.461446  0.824273                0.20464   0.383404  0.146999\nGBM_2_AutoML_1_20230224_110317                                            0.844232   0.464513  0.822217                0.213387  0.384974  0.148205\nXGBoost_lr_search_selection_AutoML_1_20230224_110317_select_grid_model_3  0.844084   0.474957  0.824804                0.212613  0.390422  0.152429\nStackedEnsemble_AllModels_2_AutoML_1_20230224_110317                      0.843772   0.46338   0.825211                0.203489  0.384146  0.147568\nGBM_grid_1_AutoML_1_20230224_110317_model_13                              0.843015   0.467719  0.811182                0.221129  0.387232  0.149949\nGBM_grid_1_AutoML_1_20230224_110317_model_8                               0.842447   0.468423  0.827004                0.225942  0.388615  0.151022\nGBM_grid_1_AutoML_1_20230224_110317_model_7                               0.841682   0.471494  0.824718                0.219475  0.390558  0.152535\nXGBoost_grid_1_AutoML_1_20230224_110317_model_4                           0.83872    0.472536  0.815819                0.21481   0.390805  0.152728\nGBM_grid_1_AutoML_1_20230224_110317_model_10                              0.838637   0.482206  0.818458                0.207109  0.39315   0.154567\nGBM_4_AutoML_1_20230224_110317                                            0.83858    0.473272  0.825463                0.222552  0.389296  0.151552\nXRT_1_AutoML_1_20230224_110317                                            0.83844    0.481299  0.811005                0.224017  0.393552  0.154883\nXGBoost_grid_1_AutoML_1_20230224_110317_model_6                           0.838218   0.473903  0.81466                 0.209725  0.391139  0.15299\nGBM_3_AutoML_1_20230224_110317                                            0.837025   0.475101  0.827295                0.218471  0.390174  0.152235\nStackedEnsemble_BestOfFamily_2_AutoML_1_20230224_110317                   0.836836   0.470399  0.822669                0.212885  0.388118  0.150635\nStackedEnsemble_AllModels_1_AutoML_1_20230224_110317                      0.836704   0.473125  0.819971                0.215312  0.389533  0.151736\nXGBoost_grid_1_AutoML_1_20230224_110317_model_1                           0.836556   0.479551  0.815375                0.209997  0.392335  0.153926\nXGBoost_grid_1_AutoML_1_20230224_110317_model_2                           0.836095   0.47282   0.816365                0.222635  0.391354  0.153158\nGBM_grid_1_AutoML_1_20230224_110317_model_2                               0.835766   0.478727  0.81786                 0.226987  0.392962  0.154419\nXGBoost_2_AutoML_1_20230224_110317                                        0.835132   0.478313  0.815482                0.229102  0.395152  0.156145\nGBM_grid_1_AutoML_1_20230224_110317_model_11                              0.834877   0.477271  0.80494                 0.220397  0.392948  0.154408\nXGBoost_grid_1_AutoML_1_20230224_110317_model_10                          0.834499   0.474252  0.8226                  0.208573  0.391206  0.153042\nGBM_grid_1_AutoML_1_20230224_110317_model_1                               0.834022   0.476102  0.814783                0.222552  0.39065   0.152607\nGBM_grid_1_AutoML_1_20230224_110317_model_5                               0.83375    0.48712   0.813883                0.226674  0.396811  0.157459\nXGBoost_grid_1_AutoML_1_20230224_110317_model_12                          0.833726   0.490951  0.812854                0.221318  0.394149  0.155354\nXGBoost_grid_1_AutoML_1_20230224_110317_model_7                           0.833388   0.50125   0.809311                0.219664  0.397723  0.158183\nDeepLearning_grid_1_AutoML_1_20230224_110317_model_4                      0.833108   0.502029  0.814078                0.216274  0.396637  0.157321\nStackedEnsemble_AllModels_4_AutoML_1_20230224_110317                      0.832919   0.470392  0.817489                0.204139  0.387134  0.149873\nGBM_grid_1_AutoML_1_20230224_110317_model_3                               0.832039   0.49615   0.815647                0.225481  0.40075   0.160601\nStackedEnsemble_AllModels_3_AutoML_1_20230224_110317                      0.831298   0.476683  0.813625                0.209223  0.389293  0.151549\nDRF_1_AutoML_1_20230224_110317                                            0.830846   0.503376  0.809738                0.236342  0.400353  0.160283\nGBM_grid_1_AutoML_1_20230224_110317_model_6                               0.829998   0.490926  0.81156                 0.213156  0.397998  0.158402\nXGBoost_grid_1_AutoML_1_20230224_110317_model_5                           0.829719   0.478294  0.81729                 0.225292  0.393961  0.155205\nGBM_5_AutoML_1_20230224_110317                                            0.825539   0.490322  0.804828                0.226674  0.398567  0.158856\nXGBoost_grid_1_AutoML_1_20230224_110317_model_11                          0.825267   0.536308  0.804757                0.218471  0.407108  0.165737\nXGBoost_grid_1_AutoML_1_20230224_110317_model_8                           0.825095   0.481637  0.810383                0.231027  0.395052  0.156066\nGLM_1_AutoML_1_20230224_110317                                            0.824905   0.491863  0.806519                0.237305  0.399719  0.159775\nDeepLearning_grid_1_AutoML_1_20230224_110317_model_1                      0.824848   0.891851  0.801048                0.221359  0.420245  0.176606\nXGBoost_grid_1_AutoML_1_20230224_110317_model_13                          0.822149   0.530776  0.794174                0.228871  0.408147  0.166584\nStackedEnsemble_BestOfFamily_1_AutoML_1_20230224_110317                   0.819788   0.493709  0.80193                 0.234499  0.400703  0.160563\nDeepLearning_grid_1_AutoML_1_20230224_110317_model_3                      0.819631   0.499773  0.814999                0.222281  0.400625  0.1605\nDeepLearning_grid_1_AutoML_1_20230224_110317_model_2                      0.818981   0.500932  0.803587                0.232639  0.395193  0.156178\nGBM_grid_1_AutoML_1_20230224_110317_model_4                               0.818957   0.512941  0.795522                0.246281  0.409159  0.167411\nDeepLearning_1_AutoML_1_20230224_110317                                   0.816743   0.508498  0.784867                0.229332  0.406192  0.164992\nDeepLearning_grid_2_AutoML_1_20230224_110317_model_1                      0.814974   0.515724  0.789822                0.238308  0.404233  0.163404\nXGBoost_grid_1_AutoML_1_20230224_110317_model_9                           0.810655   0.503262  0.796134                0.232574  0.405254  0.164231\nXGBoost_grid_1_AutoML_1_20230224_110317_model_15                          0.810186   0.498551  0.800246                0.243706  0.403429  0.162755\nDeepLearning_grid_1_AutoML_1_20230224_110317_model_6                      0.809503   0.537675  0.760926                0.261535  0.418667  0.175282\nXGBoost_1_AutoML_1_20230224_110317                                        0.809322   0.500488  0.794528                0.242932  0.404295  0.163455\nGBM_1_AutoML_1_20230224_110317                                            0.808516   0.515213  0.79556                 0.268504  0.41035   0.168387\nDeepLearning_grid_1_AutoML_1_20230224_110317_model_5                      0.807602   0.558995  0.799871                0.230879  0.41092   0.168855\nDeepLearning_grid_2_AutoML_1_20230224_110317_model_3                      0.806961   0.589088  0.773788                0.260844  0.421605  0.177751\nDeepLearning_grid_3_AutoML_1_20230224_110317_model_2                      0.802353   0.617038  0.785378                0.238851  0.414661  0.171944\nXGBoost_grid_1_AutoML_1_20230224_110317_model_14                          0.801687   0.508089  0.794118                0.251678  0.407376  0.165955\nXGBoost_grid_1_AutoML_1_20230224_110317_model_3                           0.799062   0.519443  0.784831                0.267163  0.412607  0.170245\nDeepLearning_grid_3_AutoML_1_20230224_110317_model_4                      0.792784   0.564428  0.751127                0.257224  0.430127  0.18501\nDeepLearning_grid_3_AutoML_1_20230224_110317_model_1                      0.789493   0.635284  0.753236                0.235503  0.433038  0.187522\nDeepLearning_grid_2_AutoML_1_20230224_110317_model_2                      0.786638   0.557836  0.769457                0.259042  0.421259  0.177459\nDeepLearning_grid_1_AutoML_1_20230224_110317_model_7                      0.776625   0.595141  0.736808                0.25938   0.434537  0.188823\nDeepLearning_grid_3_AutoML_1_20230224_110317_model_3                      0.735675   0.677652  0.735244                0.347203  0.457554  0.209356\nDeepLearning_grid_2_AutoML_1_20230224_110317_model_4                      0.727365   0.604306  0.655681                0.321359  0.453632  0.205782\n[63 rows x 7 columns]\n",
      "text/html": "<table class='dataframe'>\n<thead>\n<tr><th>model_id                                                                </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n</thead>\n<tbody>\n<tr><td>StackedEnsemble_BestOfFamily_3_AutoML_1_20230224_110317                 </td><td style=\"text-align: right;\">0.846133</td><td style=\"text-align: right;\"> 0.459356</td><td style=\"text-align: right;\">0.829293</td><td style=\"text-align: right;\">              0.203176</td><td style=\"text-align: right;\">0.382731</td><td style=\"text-align: right;\">0.146483</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_12                            </td><td style=\"text-align: right;\">0.845606</td><td style=\"text-align: right;\"> 0.465041</td><td style=\"text-align: right;\">0.827992</td><td style=\"text-align: right;\">              0.220166</td><td style=\"text-align: right;\">0.386294</td><td style=\"text-align: right;\">0.149223</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_9                             </td><td style=\"text-align: right;\">0.844734</td><td style=\"text-align: right;\"> 0.458622</td><td style=\"text-align: right;\">0.831179</td><td style=\"text-align: right;\">              0.208532</td><td style=\"text-align: right;\">0.382822</td><td style=\"text-align: right;\">0.146552</td></tr>\n<tr><td>StackedEnsemble_BestOfFamily_4_AutoML_1_20230224_110317                 </td><td style=\"text-align: right;\">0.844405</td><td style=\"text-align: right;\"> 0.461446</td><td style=\"text-align: right;\">0.824273</td><td style=\"text-align: right;\">              0.20464 </td><td style=\"text-align: right;\">0.383404</td><td style=\"text-align: right;\">0.146999</td></tr>\n<tr><td>GBM_2_AutoML_1_20230224_110317                                          </td><td style=\"text-align: right;\">0.844232</td><td style=\"text-align: right;\"> 0.464513</td><td style=\"text-align: right;\">0.822217</td><td style=\"text-align: right;\">              0.213387</td><td style=\"text-align: right;\">0.384974</td><td style=\"text-align: right;\">0.148205</td></tr>\n<tr><td>XGBoost_lr_search_selection_AutoML_1_20230224_110317_select_grid_model_3</td><td style=\"text-align: right;\">0.844084</td><td style=\"text-align: right;\"> 0.474957</td><td style=\"text-align: right;\">0.824804</td><td style=\"text-align: right;\">              0.212613</td><td style=\"text-align: right;\">0.390422</td><td style=\"text-align: right;\">0.152429</td></tr>\n<tr><td>StackedEnsemble_AllModels_2_AutoML_1_20230224_110317                    </td><td style=\"text-align: right;\">0.843772</td><td style=\"text-align: right;\"> 0.46338 </td><td style=\"text-align: right;\">0.825211</td><td style=\"text-align: right;\">              0.203489</td><td style=\"text-align: right;\">0.384146</td><td style=\"text-align: right;\">0.147568</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_13                            </td><td style=\"text-align: right;\">0.843015</td><td style=\"text-align: right;\"> 0.467719</td><td style=\"text-align: right;\">0.811182</td><td style=\"text-align: right;\">              0.221129</td><td style=\"text-align: right;\">0.387232</td><td style=\"text-align: right;\">0.149949</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_8                             </td><td style=\"text-align: right;\">0.842447</td><td style=\"text-align: right;\"> 0.468423</td><td style=\"text-align: right;\">0.827004</td><td style=\"text-align: right;\">              0.225942</td><td style=\"text-align: right;\">0.388615</td><td style=\"text-align: right;\">0.151022</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_7                             </td><td style=\"text-align: right;\">0.841682</td><td style=\"text-align: right;\"> 0.471494</td><td style=\"text-align: right;\">0.824718</td><td style=\"text-align: right;\">              0.219475</td><td style=\"text-align: right;\">0.390558</td><td style=\"text-align: right;\">0.152535</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_4                         </td><td style=\"text-align: right;\">0.83872 </td><td style=\"text-align: right;\"> 0.472536</td><td style=\"text-align: right;\">0.815819</td><td style=\"text-align: right;\">              0.21481 </td><td style=\"text-align: right;\">0.390805</td><td style=\"text-align: right;\">0.152728</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_10                            </td><td style=\"text-align: right;\">0.838637</td><td style=\"text-align: right;\"> 0.482206</td><td style=\"text-align: right;\">0.818458</td><td style=\"text-align: right;\">              0.207109</td><td style=\"text-align: right;\">0.39315 </td><td style=\"text-align: right;\">0.154567</td></tr>\n<tr><td>GBM_4_AutoML_1_20230224_110317                                          </td><td style=\"text-align: right;\">0.83858 </td><td style=\"text-align: right;\"> 0.473272</td><td style=\"text-align: right;\">0.825463</td><td style=\"text-align: right;\">              0.222552</td><td style=\"text-align: right;\">0.389296</td><td style=\"text-align: right;\">0.151552</td></tr>\n<tr><td>XRT_1_AutoML_1_20230224_110317                                          </td><td style=\"text-align: right;\">0.83844 </td><td style=\"text-align: right;\"> 0.481299</td><td style=\"text-align: right;\">0.811005</td><td style=\"text-align: right;\">              0.224017</td><td style=\"text-align: right;\">0.393552</td><td style=\"text-align: right;\">0.154883</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_6                         </td><td style=\"text-align: right;\">0.838218</td><td style=\"text-align: right;\"> 0.473903</td><td style=\"text-align: right;\">0.81466 </td><td style=\"text-align: right;\">              0.209725</td><td style=\"text-align: right;\">0.391139</td><td style=\"text-align: right;\">0.15299 </td></tr>\n<tr><td>GBM_3_AutoML_1_20230224_110317                                          </td><td style=\"text-align: right;\">0.837025</td><td style=\"text-align: right;\"> 0.475101</td><td style=\"text-align: right;\">0.827295</td><td style=\"text-align: right;\">              0.218471</td><td style=\"text-align: right;\">0.390174</td><td style=\"text-align: right;\">0.152235</td></tr>\n<tr><td>StackedEnsemble_BestOfFamily_2_AutoML_1_20230224_110317                 </td><td style=\"text-align: right;\">0.836836</td><td style=\"text-align: right;\"> 0.470399</td><td style=\"text-align: right;\">0.822669</td><td style=\"text-align: right;\">              0.212885</td><td style=\"text-align: right;\">0.388118</td><td style=\"text-align: right;\">0.150635</td></tr>\n<tr><td>StackedEnsemble_AllModels_1_AutoML_1_20230224_110317                    </td><td style=\"text-align: right;\">0.836704</td><td style=\"text-align: right;\"> 0.473125</td><td style=\"text-align: right;\">0.819971</td><td style=\"text-align: right;\">              0.215312</td><td style=\"text-align: right;\">0.389533</td><td style=\"text-align: right;\">0.151736</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_1                         </td><td style=\"text-align: right;\">0.836556</td><td style=\"text-align: right;\"> 0.479551</td><td style=\"text-align: right;\">0.815375</td><td style=\"text-align: right;\">              0.209997</td><td style=\"text-align: right;\">0.392335</td><td style=\"text-align: right;\">0.153926</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_2                         </td><td style=\"text-align: right;\">0.836095</td><td style=\"text-align: right;\"> 0.47282 </td><td style=\"text-align: right;\">0.816365</td><td style=\"text-align: right;\">              0.222635</td><td style=\"text-align: right;\">0.391354</td><td style=\"text-align: right;\">0.153158</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_2                             </td><td style=\"text-align: right;\">0.835766</td><td style=\"text-align: right;\"> 0.478727</td><td style=\"text-align: right;\">0.81786 </td><td style=\"text-align: right;\">              0.226987</td><td style=\"text-align: right;\">0.392962</td><td style=\"text-align: right;\">0.154419</td></tr>\n<tr><td>XGBoost_2_AutoML_1_20230224_110317                                      </td><td style=\"text-align: right;\">0.835132</td><td style=\"text-align: right;\"> 0.478313</td><td style=\"text-align: right;\">0.815482</td><td style=\"text-align: right;\">              0.229102</td><td style=\"text-align: right;\">0.395152</td><td style=\"text-align: right;\">0.156145</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_11                            </td><td style=\"text-align: right;\">0.834877</td><td style=\"text-align: right;\"> 0.477271</td><td style=\"text-align: right;\">0.80494 </td><td style=\"text-align: right;\">              0.220397</td><td style=\"text-align: right;\">0.392948</td><td style=\"text-align: right;\">0.154408</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_10                        </td><td style=\"text-align: right;\">0.834499</td><td style=\"text-align: right;\"> 0.474252</td><td style=\"text-align: right;\">0.8226  </td><td style=\"text-align: right;\">              0.208573</td><td style=\"text-align: right;\">0.391206</td><td style=\"text-align: right;\">0.153042</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_1                             </td><td style=\"text-align: right;\">0.834022</td><td style=\"text-align: right;\"> 0.476102</td><td style=\"text-align: right;\">0.814783</td><td style=\"text-align: right;\">              0.222552</td><td style=\"text-align: right;\">0.39065 </td><td style=\"text-align: right;\">0.152607</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_5                             </td><td style=\"text-align: right;\">0.83375 </td><td style=\"text-align: right;\"> 0.48712 </td><td style=\"text-align: right;\">0.813883</td><td style=\"text-align: right;\">              0.226674</td><td style=\"text-align: right;\">0.396811</td><td style=\"text-align: right;\">0.157459</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_12                        </td><td style=\"text-align: right;\">0.833726</td><td style=\"text-align: right;\"> 0.490951</td><td style=\"text-align: right;\">0.812854</td><td style=\"text-align: right;\">              0.221318</td><td style=\"text-align: right;\">0.394149</td><td style=\"text-align: right;\">0.155354</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_7                         </td><td style=\"text-align: right;\">0.833388</td><td style=\"text-align: right;\"> 0.50125 </td><td style=\"text-align: right;\">0.809311</td><td style=\"text-align: right;\">              0.219664</td><td style=\"text-align: right;\">0.397723</td><td style=\"text-align: right;\">0.158183</td></tr>\n<tr><td>DeepLearning_grid_1_AutoML_1_20230224_110317_model_4                    </td><td style=\"text-align: right;\">0.833108</td><td style=\"text-align: right;\"> 0.502029</td><td style=\"text-align: right;\">0.814078</td><td style=\"text-align: right;\">              0.216274</td><td style=\"text-align: right;\">0.396637</td><td style=\"text-align: right;\">0.157321</td></tr>\n<tr><td>StackedEnsemble_AllModels_4_AutoML_1_20230224_110317                    </td><td style=\"text-align: right;\">0.832919</td><td style=\"text-align: right;\"> 0.470392</td><td style=\"text-align: right;\">0.817489</td><td style=\"text-align: right;\">              0.204139</td><td style=\"text-align: right;\">0.387134</td><td style=\"text-align: right;\">0.149873</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_3                             </td><td style=\"text-align: right;\">0.832039</td><td style=\"text-align: right;\"> 0.49615 </td><td style=\"text-align: right;\">0.815647</td><td style=\"text-align: right;\">              0.225481</td><td style=\"text-align: right;\">0.40075 </td><td style=\"text-align: right;\">0.160601</td></tr>\n<tr><td>StackedEnsemble_AllModels_3_AutoML_1_20230224_110317                    </td><td style=\"text-align: right;\">0.831298</td><td style=\"text-align: right;\"> 0.476683</td><td style=\"text-align: right;\">0.813625</td><td style=\"text-align: right;\">              0.209223</td><td style=\"text-align: right;\">0.389293</td><td style=\"text-align: right;\">0.151549</td></tr>\n<tr><td>DRF_1_AutoML_1_20230224_110317                                          </td><td style=\"text-align: right;\">0.830846</td><td style=\"text-align: right;\"> 0.503376</td><td style=\"text-align: right;\">0.809738</td><td style=\"text-align: right;\">              0.236342</td><td style=\"text-align: right;\">0.400353</td><td style=\"text-align: right;\">0.160283</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_6                             </td><td style=\"text-align: right;\">0.829998</td><td style=\"text-align: right;\"> 0.490926</td><td style=\"text-align: right;\">0.81156 </td><td style=\"text-align: right;\">              0.213156</td><td style=\"text-align: right;\">0.397998</td><td style=\"text-align: right;\">0.158402</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_5                         </td><td style=\"text-align: right;\">0.829719</td><td style=\"text-align: right;\"> 0.478294</td><td style=\"text-align: right;\">0.81729 </td><td style=\"text-align: right;\">              0.225292</td><td style=\"text-align: right;\">0.393961</td><td style=\"text-align: right;\">0.155205</td></tr>\n<tr><td>GBM_5_AutoML_1_20230224_110317                                          </td><td style=\"text-align: right;\">0.825539</td><td style=\"text-align: right;\"> 0.490322</td><td style=\"text-align: right;\">0.804828</td><td style=\"text-align: right;\">              0.226674</td><td style=\"text-align: right;\">0.398567</td><td style=\"text-align: right;\">0.158856</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_11                        </td><td style=\"text-align: right;\">0.825267</td><td style=\"text-align: right;\"> 0.536308</td><td style=\"text-align: right;\">0.804757</td><td style=\"text-align: right;\">              0.218471</td><td style=\"text-align: right;\">0.407108</td><td style=\"text-align: right;\">0.165737</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_8                         </td><td style=\"text-align: right;\">0.825095</td><td style=\"text-align: right;\"> 0.481637</td><td style=\"text-align: right;\">0.810383</td><td style=\"text-align: right;\">              0.231027</td><td style=\"text-align: right;\">0.395052</td><td style=\"text-align: right;\">0.156066</td></tr>\n<tr><td>GLM_1_AutoML_1_20230224_110317                                          </td><td style=\"text-align: right;\">0.824905</td><td style=\"text-align: right;\"> 0.491863</td><td style=\"text-align: right;\">0.806519</td><td style=\"text-align: right;\">              0.237305</td><td style=\"text-align: right;\">0.399719</td><td style=\"text-align: right;\">0.159775</td></tr>\n<tr><td>DeepLearning_grid_1_AutoML_1_20230224_110317_model_1                    </td><td style=\"text-align: right;\">0.824848</td><td style=\"text-align: right;\"> 0.891851</td><td style=\"text-align: right;\">0.801048</td><td style=\"text-align: right;\">              0.221359</td><td style=\"text-align: right;\">0.420245</td><td style=\"text-align: right;\">0.176606</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_13                        </td><td style=\"text-align: right;\">0.822149</td><td style=\"text-align: right;\"> 0.530776</td><td style=\"text-align: right;\">0.794174</td><td style=\"text-align: right;\">              0.228871</td><td style=\"text-align: right;\">0.408147</td><td style=\"text-align: right;\">0.166584</td></tr>\n<tr><td>StackedEnsemble_BestOfFamily_1_AutoML_1_20230224_110317                 </td><td style=\"text-align: right;\">0.819788</td><td style=\"text-align: right;\"> 0.493709</td><td style=\"text-align: right;\">0.80193 </td><td style=\"text-align: right;\">              0.234499</td><td style=\"text-align: right;\">0.400703</td><td style=\"text-align: right;\">0.160563</td></tr>\n<tr><td>DeepLearning_grid_1_AutoML_1_20230224_110317_model_3                    </td><td style=\"text-align: right;\">0.819631</td><td style=\"text-align: right;\"> 0.499773</td><td style=\"text-align: right;\">0.814999</td><td style=\"text-align: right;\">              0.222281</td><td style=\"text-align: right;\">0.400625</td><td style=\"text-align: right;\">0.1605  </td></tr>\n<tr><td>DeepLearning_grid_1_AutoML_1_20230224_110317_model_2                    </td><td style=\"text-align: right;\">0.818981</td><td style=\"text-align: right;\"> 0.500932</td><td style=\"text-align: right;\">0.803587</td><td style=\"text-align: right;\">              0.232639</td><td style=\"text-align: right;\">0.395193</td><td style=\"text-align: right;\">0.156178</td></tr>\n<tr><td>GBM_grid_1_AutoML_1_20230224_110317_model_4                             </td><td style=\"text-align: right;\">0.818957</td><td style=\"text-align: right;\"> 0.512941</td><td style=\"text-align: right;\">0.795522</td><td style=\"text-align: right;\">              0.246281</td><td style=\"text-align: right;\">0.409159</td><td style=\"text-align: right;\">0.167411</td></tr>\n<tr><td>DeepLearning_1_AutoML_1_20230224_110317                                 </td><td style=\"text-align: right;\">0.816743</td><td style=\"text-align: right;\"> 0.508498</td><td style=\"text-align: right;\">0.784867</td><td style=\"text-align: right;\">              0.229332</td><td style=\"text-align: right;\">0.406192</td><td style=\"text-align: right;\">0.164992</td></tr>\n<tr><td>DeepLearning_grid_2_AutoML_1_20230224_110317_model_1                    </td><td style=\"text-align: right;\">0.814974</td><td style=\"text-align: right;\"> 0.515724</td><td style=\"text-align: right;\">0.789822</td><td style=\"text-align: right;\">              0.238308</td><td style=\"text-align: right;\">0.404233</td><td style=\"text-align: right;\">0.163404</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_9                         </td><td style=\"text-align: right;\">0.810655</td><td style=\"text-align: right;\"> 0.503262</td><td style=\"text-align: right;\">0.796134</td><td style=\"text-align: right;\">              0.232574</td><td style=\"text-align: right;\">0.405254</td><td style=\"text-align: right;\">0.164231</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_15                        </td><td style=\"text-align: right;\">0.810186</td><td style=\"text-align: right;\"> 0.498551</td><td style=\"text-align: right;\">0.800246</td><td style=\"text-align: right;\">              0.243706</td><td style=\"text-align: right;\">0.403429</td><td style=\"text-align: right;\">0.162755</td></tr>\n<tr><td>DeepLearning_grid_1_AutoML_1_20230224_110317_model_6                    </td><td style=\"text-align: right;\">0.809503</td><td style=\"text-align: right;\"> 0.537675</td><td style=\"text-align: right;\">0.760926</td><td style=\"text-align: right;\">              0.261535</td><td style=\"text-align: right;\">0.418667</td><td style=\"text-align: right;\">0.175282</td></tr>\n<tr><td>XGBoost_1_AutoML_1_20230224_110317                                      </td><td style=\"text-align: right;\">0.809322</td><td style=\"text-align: right;\"> 0.500488</td><td style=\"text-align: right;\">0.794528</td><td style=\"text-align: right;\">              0.242932</td><td style=\"text-align: right;\">0.404295</td><td style=\"text-align: right;\">0.163455</td></tr>\n<tr><td>GBM_1_AutoML_1_20230224_110317                                          </td><td style=\"text-align: right;\">0.808516</td><td style=\"text-align: right;\"> 0.515213</td><td style=\"text-align: right;\">0.79556 </td><td style=\"text-align: right;\">              0.268504</td><td style=\"text-align: right;\">0.41035 </td><td style=\"text-align: right;\">0.168387</td></tr>\n<tr><td>DeepLearning_grid_1_AutoML_1_20230224_110317_model_5                    </td><td style=\"text-align: right;\">0.807602</td><td style=\"text-align: right;\"> 0.558995</td><td style=\"text-align: right;\">0.799871</td><td style=\"text-align: right;\">              0.230879</td><td style=\"text-align: right;\">0.41092 </td><td style=\"text-align: right;\">0.168855</td></tr>\n<tr><td>DeepLearning_grid_2_AutoML_1_20230224_110317_model_3                    </td><td style=\"text-align: right;\">0.806961</td><td style=\"text-align: right;\"> 0.589088</td><td style=\"text-align: right;\">0.773788</td><td style=\"text-align: right;\">              0.260844</td><td style=\"text-align: right;\">0.421605</td><td style=\"text-align: right;\">0.177751</td></tr>\n<tr><td>DeepLearning_grid_3_AutoML_1_20230224_110317_model_2                    </td><td style=\"text-align: right;\">0.802353</td><td style=\"text-align: right;\"> 0.617038</td><td style=\"text-align: right;\">0.785378</td><td style=\"text-align: right;\">              0.238851</td><td style=\"text-align: right;\">0.414661</td><td style=\"text-align: right;\">0.171944</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_14                        </td><td style=\"text-align: right;\">0.801687</td><td style=\"text-align: right;\"> 0.508089</td><td style=\"text-align: right;\">0.794118</td><td style=\"text-align: right;\">              0.251678</td><td style=\"text-align: right;\">0.407376</td><td style=\"text-align: right;\">0.165955</td></tr>\n<tr><td>XGBoost_grid_1_AutoML_1_20230224_110317_model_3                         </td><td style=\"text-align: right;\">0.799062</td><td style=\"text-align: right;\"> 0.519443</td><td style=\"text-align: right;\">0.784831</td><td style=\"text-align: right;\">              0.267163</td><td style=\"text-align: right;\">0.412607</td><td style=\"text-align: right;\">0.170245</td></tr>\n<tr><td>DeepLearning_grid_3_AutoML_1_20230224_110317_model_4                    </td><td style=\"text-align: right;\">0.792784</td><td style=\"text-align: right;\"> 0.564428</td><td style=\"text-align: right;\">0.751127</td><td style=\"text-align: right;\">              0.257224</td><td style=\"text-align: right;\">0.430127</td><td style=\"text-align: right;\">0.18501 </td></tr>\n<tr><td>DeepLearning_grid_3_AutoML_1_20230224_110317_model_1                    </td><td style=\"text-align: right;\">0.789493</td><td style=\"text-align: right;\"> 0.635284</td><td style=\"text-align: right;\">0.753236</td><td style=\"text-align: right;\">              0.235503</td><td style=\"text-align: right;\">0.433038</td><td style=\"text-align: right;\">0.187522</td></tr>\n<tr><td>DeepLearning_grid_2_AutoML_1_20230224_110317_model_2                    </td><td style=\"text-align: right;\">0.786638</td><td style=\"text-align: right;\"> 0.557836</td><td style=\"text-align: right;\">0.769457</td><td style=\"text-align: right;\">              0.259042</td><td style=\"text-align: right;\">0.421259</td><td style=\"text-align: right;\">0.177459</td></tr>\n<tr><td>DeepLearning_grid_1_AutoML_1_20230224_110317_model_7                    </td><td style=\"text-align: right;\">0.776625</td><td style=\"text-align: right;\"> 0.595141</td><td style=\"text-align: right;\">0.736808</td><td style=\"text-align: right;\">              0.25938 </td><td style=\"text-align: right;\">0.434537</td><td style=\"text-align: right;\">0.188823</td></tr>\n<tr><td>DeepLearning_grid_3_AutoML_1_20230224_110317_model_3                    </td><td style=\"text-align: right;\">0.735675</td><td style=\"text-align: right;\"> 0.677652</td><td style=\"text-align: right;\">0.735244</td><td style=\"text-align: right;\">              0.347203</td><td style=\"text-align: right;\">0.457554</td><td style=\"text-align: right;\">0.209356</td></tr>\n<tr><td>DeepLearning_grid_2_AutoML_1_20230224_110317_model_4                    </td><td style=\"text-align: right;\">0.727365</td><td style=\"text-align: right;\"> 0.604306</td><td style=\"text-align: right;\">0.655681</td><td style=\"text-align: right;\">              0.321359</td><td style=\"text-align: right;\">0.453632</td><td style=\"text-align: right;\">0.205782</td></tr>\n</tbody>\n</table><pre style='font-size: smaller; margin-bottom: 1em;'>[63 rows x 7 columns]</pre>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |███████████████████████████████████████████| (done) 100%\n",
      "stackedensemble prediction progress: |███████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": "  predict        p0         p1\n---------  --------  ---------\n        0  0.931376  0.0686239\n        0  0.733829  0.266171\n        1  0.412671  0.587329\n        1  0.329067  0.670933\n        1  0.275716  0.724284\n        0  0.940674  0.0593263\n        1  0.4401    0.5599\n        0  0.887337  0.112663\n        1  0.296694  0.703306\n        0  0.87071   0.12929\n[211 rows x 3 columns]\n",
      "text/html": "<table class='dataframe'>\n<thead>\n<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">      p0</th><th style=\"text-align: right;\">       p1</th></tr>\n</thead>\n<tbody>\n<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.931376</td><td style=\"text-align: right;\">0.0686239</td></tr>\n<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.733829</td><td style=\"text-align: right;\">0.266171 </td></tr>\n<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.412671</td><td style=\"text-align: right;\">0.587329 </td></tr>\n<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.329067</td><td style=\"text-align: right;\">0.670933 </td></tr>\n<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.275716</td><td style=\"text-align: right;\">0.724284 </td></tr>\n<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.940674</td><td style=\"text-align: right;\">0.0593263</td></tr>\n<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.4401  </td><td style=\"text-align: right;\">0.5599   </td></tr>\n<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.887337</td><td style=\"text-align: right;\">0.112663 </td></tr>\n<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.296694</td><td style=\"text-align: right;\">0.703306 </td></tr>\n<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.87071 </td><td style=\"text-align: right;\">0.12929  </td></tr>\n</tbody>\n</table><pre style='font-size: smaller; margin-bottom: 1em;'>[211 rows x 3 columns]</pre>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = aml.leader.predict(test)\n",
    "preds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "{'model_id': {'default': None,\n  'actual': {'__meta': {'schema_version': 3,\n    'schema_name': 'ModelKeyV3',\n    'schema_type': 'Key<Model>'},\n   'name': 'StackedEnsemble_BestOfFamily_3_AutoML_1_20230224_110317',\n   'type': 'Key<Model>',\n   'URL': '/3/Models/StackedEnsemble_BestOfFamily_3_AutoML_1_20230224_110317'},\n  'input': None},\n 'training_frame': {'default': None,\n  'actual': {'__meta': {'schema_version': 3,\n    'schema_name': 'FrameKeyV3',\n    'schema_type': 'Key<Frame>'},\n   'name': 'AutoML_1_20230224_110317_training_py_12_sid_b7bb',\n   'type': 'Key<Frame>',\n   'URL': '/3/Frames/AutoML_1_20230224_110317_training_py_12_sid_b7bb'},\n  'input': {'__meta': {'schema_version': 3,\n    'schema_name': 'FrameKeyV3',\n    'schema_type': 'Key<Frame>'},\n   'name': 'AutoML_1_20230224_110317_training_py_12_sid_b7bb',\n   'type': 'Key<Frame>',\n   'URL': '/3/Frames/AutoML_1_20230224_110317_training_py_12_sid_b7bb'}},\n 'response_column': {'default': None,\n  'actual': {'__meta': {'schema_version': 3,\n    'schema_name': 'ColSpecifierV3',\n    'schema_type': 'VecSpecifier'},\n   'column_name': 'Survived',\n   'is_member_of_frames': None},\n  'input': {'__meta': {'schema_version': 3,\n    'schema_name': 'ColSpecifierV3',\n    'schema_type': 'VecSpecifier'},\n   'column_name': 'Survived',\n   'is_member_of_frames': None}},\n 'validation_frame': {'default': None,\n  'actual': {'__meta': {'schema_version': 3,\n    'schema_name': 'FrameKeyV3',\n    'schema_type': 'Key<Frame>'},\n   'name': 'py_10_sid_b7bb',\n   'type': 'Key<Frame>',\n   'URL': '/3/Frames/py_10_sid_b7bb'},\n  'input': {'__meta': {'schema_version': 3,\n    'schema_name': 'FrameKeyV3',\n    'schema_type': 'Key<Frame>'},\n   'name': 'py_10_sid_b7bb',\n   'type': 'Key<Frame>',\n   'URL': '/3/Frames/py_10_sid_b7bb'}},\n 'blending_frame': {'default': None, 'actual': None, 'input': None},\n 'base_models': {'default': [],\n  'actual': [{'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'GBM_2_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None},\n   {'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'XGBoost_3_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None},\n   {'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'XRT_1_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None},\n   {'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'DRF_1_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None},\n   {'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'GLM_1_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None},\n   {'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'DeepLearning_1_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None}],\n  'input': [{'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'GBM_2_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None},\n   {'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'XGBoost_3_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None},\n   {'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'XRT_1_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None},\n   {'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'DRF_1_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None},\n   {'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'GLM_1_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None},\n   {'__meta': {'schema_version': 3,\n     'schema_name': 'KeyV3',\n     'schema_type': 'Key<Keyed>'},\n    'name': 'DeepLearning_1_AutoML_1_20230224_110317',\n    'type': 'Key<Keyed>',\n    'URL': None}]},\n 'metalearner_algorithm': {'default': 'AUTO',\n  'actual': 'glm',\n  'input': 'AUTO'},\n 'metalearner_nfolds': {'default': 0, 'actual': 5, 'input': 5},\n 'metalearner_fold_assignment': {'default': None,\n  'actual': None,\n  'input': None},\n 'metalearner_fold_column': {'default': None, 'actual': None, 'input': None},\n 'metalearner_params': {'default': '', 'actual': '', 'input': ''},\n 'metalearner_transform': {'default': 'NONE',\n  'actual': 'Logit',\n  'input': 'Logit'},\n 'max_runtime_secs': {'default': 0.0, 'actual': 17.563, 'input': 17.563},\n 'weights_column': {'default': None, 'actual': None, 'input': None},\n 'offset_column': {'default': None, 'actual': None, 'input': None},\n 'seed': {'default': -1, 'actual': 1465854493307313537, 'input': -1},\n 'score_training_samples': {'default': 10000, 'actual': 10000, 'input': 10000},\n 'keep_levelone_frame': {'default': False, 'actual': True, 'input': True},\n 'export_checkpoints_dir': {'default': None, 'actual': None, 'input': None},\n 'auc_type': {'default': 'AUTO', 'actual': 'AUTO', 'input': 'AUTO'}}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model_path = h2o.save_model(model=aml.leader, path=\"./mymodel\", force=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "Model Details\n=============\nH2OStackedEnsembleEstimator : Stacked Ensemble\nModel Key: StackedEnsemble_BestOfFamily_3_AutoML_1_20230224_110317\n\n\nModel Summary for Stacked Ensemble: \nkey                                        value\n-----------------------------------------  ----------------\nStacking strategy                          cross_validation\nNumber of base models (used / total)       6/6\n# GBM base models (used / total)           1/1\n# XGBoost base models (used / total)       1/1\n# DRF base models (used / total)           2/2\n# GLM base models (used / total)           1/1\n# DeepLearning base models (used / total)  1/1\nMetalearner algorithm                      GLM\nMetalearner fold assignment scheme         Random\nMetalearner nfolds                         5\nMetalearner fold_column\nCustom metalearner hyperparameters         None\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.090302569991916\nRMSE: 0.3005038601947003\nLogLoss: 0.30788977805421525\nAUC: 0.9528632548955077\nAUCPR: 0.9442040059021868\nGini: 0.9057265097910154\nNull degrees of freedom: 500\nResidual degrees of freedom: 494\nNull deviance: 678.6388715052319\nResidual deviance: 308.5055576103236\nAIC: 322.5055576103236\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.48698351606560014\n       0    1    Error    Rate\n-----  ---  ---  -------  ------------\n0      282  13   0.0441   (13.0/295.0)\n1      39   167  0.1893   (39.0/206.0)\nTotal  321  180  0.1038   (52.0/501.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.486984     0.865285  155\nmax f2                       0.260119     0.899814  220\nmax f0point5                 0.549071     0.911162  145\nmax accuracy                 0.486984     0.896208  155\nmax precision                0.984631     1         0\nmax recall                   0.113078     1         320\nmax specificity              0.984631     1         0\nmax absolute_mcc             0.486984     0.786197  155\nmax min_per_class_accuracy   0.341544     0.864078  190\nmax mean_per_class_accuracy  0.486984     0.883306  155\nmax tns                      0.984631     295       0\nmax fns                      0.984631     205       0\nmax fps                      0.0271721    295       399\nmax tps                      0.113078     206       320\nmax tnr                      0.984631     1         0\nmax fnr                      0.984631     0.995146  0\nmax fpr                      0.0271721    1         399\nmax tpr                      0.113078     1         320\n\nGains/Lift Table: Avg response rate: 41.12 %, avg score: 40.55 %\ngroup    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.011976                    0.978779           2.43204    2.43204            1                0.980605   1                           0.980605            0.0291262       0.0291262                  143.204   143.204            0.0291262\n2        0.0219561                   0.972523           2.43204    2.43204            1                0.976001   1                           0.978513            0.0242718       0.0533981                  143.204   143.204            0.0533981\n3        0.0319361                   0.969986           2.43204    2.43204            1                0.970856   1                           0.97612             0.0242718       0.0776699                  143.204   143.204            0.0776699\n4        0.0419162                   0.968274           2.43204    2.43204            1                0.968814   1                           0.974381            0.0242718       0.101942                   143.204   143.204            0.101942\n5        0.0518962                   0.957717           2.43204    2.43204            1                0.963536   1                           0.972295            0.0242718       0.126214                   143.204   143.204            0.126214\n6        0.101796                    0.921165           2.43204    2.43204            1                0.944738   1                           0.958787            0.121359        0.247573                   143.204   143.204            0.247573\n7        0.151697                    0.882701           2.43204    2.43204            1                0.903015   1                           0.940441            0.121359        0.368932                   143.204   143.204            0.368932\n8        0.201597                    0.820844           2.43204    2.43204            1                0.859658   1                           0.920445            0.121359        0.490291                   143.204   143.204            0.490291\n9        0.301397                    0.649236           2.14019    2.3354             0.88             0.720786   0.960265                    0.854333            0.213592        0.703883                   114.019   133.54             0.683545\n10       0.401198                    0.407514           1.21602    2.05695            0.5              0.513314   0.845771                    0.769502            0.121359        0.825243                   21.6019   105.695            0.720158\n11       0.500998                    0.26092            1.0701     1.86036            0.44             0.32208    0.76494                     0.680374            0.106796        0.932039                   7.00971   86.0364            0.732039\n12       0.600798                    0.181872           0.340485   1.60789            0.14             0.219848   0.66113                     0.603875            0.0339806       0.966019                   -65.9515  60.7893            0.620257\n13       0.700599                    0.134256           0.243204   1.41349            0.1              0.160013   0.581197                    0.540647            0.0242718       0.990291                   -75.6796  41.3493            0.491986\n14       0.800399                    0.108409           0.0972816  1.24938            0.04             0.120925   0.513716                    0.488313            0.00970874      1                          -90.2718  24.9377            0.338983\n15       0.9002                      0.0748283          0          1.11086            0                0.0924047  0.456763                    0.44442             0               1                          -100      11.0865            0.169492\n16       1                           0.0271721          0          1                  0                0.0539791  0.411178                    0.405454            0               1                          -100      0                  0\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on validation data. **\n\nMSE: 0.10790103374848198\nRMSE: 0.3284829276362502\nLogLoss: 0.3641385099423577\nAUC: 0.9066458687842692\nAUCPR: 0.8922966766719541\nGini: 0.8132917375685385\nNull degrees of freedom: 210\nResidual degrees of freedom: 204\nNull deviance: 282.39644104064814\nResidual deviance: 153.66645119567494\nAIC: 167.66645119567494\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.5038318992954588\n       0    1    Error    Rate\n-----  ---  ---  -------  ------------\n0      117  12   0.093    (12.0/129.0)\n1      15   67   0.1829   (15.0/82.0)\nTotal  132  79   0.128    (27.0/211.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.503832     0.832298  75\nmax f2                       0.310263     0.87822   95\nmax f0point5                 0.583129     0.861582  65\nmax accuracy                 0.525517     0.872038  71\nmax precision                0.978893     1         0\nmax recall                   0.0593606    1         190\nmax specificity              0.978893     1         0\nmax absolute_mcc             0.503832     0.729239  75\nmax min_per_class_accuracy   0.403435     0.844961  86\nmax mean_per_class_accuracy  0.318286     0.865948  92\nmax tns                      0.978893     129       0\nmax fns                      0.978893     81        0\nmax fps                      0.0269644    129       204\nmax tps                      0.0593606    82        190\nmax tnr                      0.978893     1         0\nmax fnr                      0.978893     0.987805  0\nmax fpr                      0.0269644    1         204\nmax tpr                      0.0593606    1         190\n\nGains/Lift Table: Avg response rate: 38.86 %, avg score: 40.50 %\ngroup    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.014218                    0.972972           2.57317   2.57317            1                0.976732   1                           0.976732            0.0365854       0.0365854                  157.317   157.317            0.0365854\n2        0.0236967                   0.958303           2.57317   2.57317            1                0.963133   1                           0.971292            0.0243902       0.0609756                  157.317   157.317            0.0609756\n3        0.0331754                   0.956284           2.57317   2.57317            1                0.956506   1                           0.967068            0.0243902       0.0853659                  157.317   157.317            0.0853659\n4        0.042654                    0.954052           2.57317   2.57317            1                0.955704   1                           0.964542            0.0243902       0.109756                   157.317   157.317            0.109756\n5        0.0521327                   0.95015            2.57317   2.57317            1                0.95134    1                           0.962142            0.0243902       0.134146                   157.317   157.317            0.134146\n6        0.104265                    0.926348           2.33925   2.45621            0.909091         0.939118   0.954545                    0.95063             0.121951        0.256098                   133.925   145.621            0.248346\n7        0.151659                    0.895439           2.57317   2.49276            1                0.912399   0.96875                     0.938683            0.121951        0.378049                   157.317   149.276            0.370297\n8        0.203791                    0.738943           2.33925   2.45349            0.909091         0.853961   0.953488                    0.91701             0.121951        0.5                        133.925   145.349            0.484496\n9        0.303318                    0.617373           1.96051   2.29173            0.761905         0.689782   0.890625                    0.842451            0.195122        0.695122                   96.0511   129.173            0.640858\n10       0.402844                    0.428148           1.34785   2.05854            0.52381          0.526222   0.8                         0.764324            0.134146        0.829268                   34.7851   105.854            0.697485\n11       0.50237                     0.255878           0.857724  1.82064            0.333333         0.335746   0.707547                    0.679417            0.0853659       0.914634                   -14.2276  82.064             0.674324\n12       0.601896                    0.1869             0.122532  1.53985            0.047619         0.220772   0.598425                    0.603578            0.0121951       0.926829                   -87.7468  53.985             0.53148\n13       0.701422                    0.132084           0.122532  1.33874            0.047619         0.162607   0.52027                     0.541008            0.0121951       0.939024                   -87.7468  33.8744            0.388637\n14       0.800948                    0.100439           0.122532  1.18762            0.047619         0.11537    0.461538                    0.488118            0.0121951       0.95122                    -87.7468  18.7617            0.245793\n15       0.900474                    0.0727439          0.245064  1.08344            0.0952381        0.0858364  0.421053                    0.443655            0.0243902       0.97561                    -75.4936  8.34403            0.122897\n16       1                           0.0269644          0.245064  1                  0.0952381        0.0548286  0.388626                    0.404957            0.0243902       1                          -75.4936  0                  0\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on cross-validation data. **\n\nMSE: 0.14648277687501338\nRMSE: 0.38273068452243725\nLogLoss: 0.4593560459236398\nAUC: 0.8461329603422741\nAUCPR: 0.8292932326677792\nGini: 0.6922659206845483\nNull degrees of freedom: 500\nResidual degrees of freedom: 494\nNull deviance: 679.6192027459856\nResidual deviance: 460.2747580154872\nAIC: 474.2747580154872\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.42980900650465503\n       0    1    Error    Rate\n-----  ---  ---  -------  -------------\n0      241  54   0.1831   (54.0/295.0)\n1      46   160  0.2233   (46.0/206.0)\nTotal  287  214  0.1996   (100.0/501.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.429809     0.761905  189\nmax f2                       0.139081     0.809801  322\nmax f0point5                 0.752813     0.807692  91\nmax accuracy                 0.515095     0.808383  156\nmax precision                0.986813     1         0\nmax recall                   0.0316588    1         399\nmax specificity              0.986813     1         0\nmax absolute_mcc             0.515095     0.600191  156\nmax min_per_class_accuracy   0.379373     0.786408  199\nmax mean_per_class_accuracy  0.429809     0.796824  189\nmax tns                      0.986813     295       0\nmax fns                      0.986813     205       0\nmax fps                      0.0354786    295       398\nmax tps                      0.0316588    206       399\nmax tnr                      0.986813     1         0\nmax fnr                      0.986813     0.995146  0\nmax fpr                      0.0354786    1         398\nmax tpr                      0.0316588    1         399\n\nGains/Lift Table: Avg response rate: 41.12 %, avg score: 41.11 %\ngroup    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.011976                    0.978794           2.43204   2.43204            1                0.982327   1                           0.982327            0.0291262       0.0291262                  143.204   143.204            0.0291262\n2        0.0219561                   0.963937           2.43204   2.43204            1                0.969741   1                           0.976606            0.0242718       0.0533981                  143.204   143.204            0.0533981\n3        0.0319361                   0.956702           2.43204   2.43204            1                0.95979    1                           0.971351            0.0242718       0.0776699                  143.204   143.204            0.0776699\n4        0.0419162                   0.949625           1.94563   2.31623            0.8              0.95188    0.952381                    0.966715            0.0194175       0.0970874                  94.5631   131.623            0.0936975\n5        0.0518962                   0.93683            2.43204   2.3385             1                0.94242    0.961538                    0.962043            0.0242718       0.121359                   143.204   133.85             0.117969\n6        0.101796                    0.906174           2.23748   2.28898            0.92             0.919991   0.941176                    0.941429            0.11165         0.23301                    123.748   128.898            0.22284\n7        0.151697                    0.847242           2.33476   2.30404            0.96             0.880731   0.947368                    0.921463            0.116505        0.349515                   133.476   130.404            0.335955\n8        0.201597                    0.775597           2.23748   2.28756            0.92             0.807483   0.940594                    0.89325             0.11165         0.461165                   123.748   128.756            0.440826\n9        0.301397                    0.608629           1.50786   2.02938            0.62             0.676915   0.834437                    0.821616            0.150485        0.61165                    50.7864   102.938            0.526905\n10       0.401198                    0.463678           1.26466   1.83915            0.52             0.5236     0.756219                    0.747483            0.126214        0.737864                   26.466    83.9154            0.571762\n11       0.500998                    0.291777           0.63233   1.59875            0.26             0.373261   0.657371                    0.672936            0.0631068       0.800971                   -36.767   59.8751            0.509445\n12       0.600798                    0.212819           0.486408  1.41398            0.2              0.242084   0.581395                    0.601366            0.0485437       0.849515                   -51.3592  41.3976            0.422396\n13       0.700599                    0.162015           0.486408  1.28184            0.2              0.185801   0.527066                    0.542169            0.0485437       0.898058                   -51.3592  28.1844            0.335346\n14       0.800399                    0.124709           0.486408  1.18266            0.2              0.142232   0.486284                    0.492301            0.0485437       0.946602                   -51.3592  18.2662            0.248297\n15       0.9002                      0.0859383          0.340485  1.08929            0.14             0.105718   0.447894                    0.449443            0.0339806       0.980583                   -65.9515  8.92946            0.136515\n16       1                           0.0316588          0.194563  1                  0.08             0.0655716  0.411178                    0.411132            0.0194175       1                          -80.5437  0                  0\n\nCross-Validation Metrics Summary: \n                      mean        sd           cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n--------------------  ----------  -----------  ------------  ------------  ------------  ------------  ------------\naccuracy              0.8231532   0.048069436  0.78          0.82954544    0.8712871     0.7676768     0.86725664\nauc                   0.8456506   0.037110217  0.85291666    0.7946623     0.8629908     0.8256198     0.8920635\nerr                   0.1768468   0.048069436  0.22          0.17045455    0.12871288    0.23232323    0.13274336\nerr_count             17.6        4.560702     22.0          15.0          13.0          23.0          15.0\nf0point5              0.796463    0.069022     0.7083333     0.8333333     0.8426966     0.73660713    0.8613445\nf1                    0.7802499   0.049913913  0.75555557    0.7368421     0.82191783    0.74157304    0.8453608\nf2                    0.7697212   0.068472974  0.8095238     0.6603774     0.80213904    0.74660635    0.8299595\nlift_top_group        2.451226    0.1876977    2.5           2.5882354     2.6578948     2.25          2.26\nlogloss               0.4596301   0.040655814  0.449594      0.50526917    0.42467046    0.49934316    0.41927382\nmax_per_class_error   0.25790918  0.07736355   0.26666668    0.38235295    0.21052632    0.25          0.18\n---                   ---         ---          ---           ---           ---           ---           ---\nmean_per_class_error  0.1869368   0.042996366  0.20833333    0.209695      0.1449457     0.23409091    0.13761905\nmse                   0.14690518  0.017054295  0.14610153    0.16552782    0.13290341    0.16249818    0.12749498\nnull_deviance         135.92384   13.548579    134.683       117.73908     134.56567     136.71875     155.9127\npr_auc                0.8313398   0.02789983   0.8331954     0.78921276    0.8502845     0.8226492     0.86135715\nprecision             0.811172    0.09937007   0.68          0.9130435     0.85714287    0.73333335    0.87234044\nr2                    0.39036426  0.07190635   0.3912436     0.301826      0.43368936    0.34188238    0.4831799\nrecall                0.76542413  0.090534456  0.85          0.61764705    0.7894737     0.75          0.82\nresidual_deviance     91.651085   5.1623745    89.9188       88.927376     85.78343      98.86994      94.75588\nrmse                  0.38276353  0.022284197  0.3822323     0.4068511     0.3645592     0.40311062    0.3570644\nspecificity           0.8607023   0.09802546   0.73333335    0.962963      0.9206349     0.7818182     0.9047619\n[22 rows x 8 columns]\n\n\n[tips]\nUse `model.explain()` to inspect the model.\n--\nUse `h2o.display.toggle_user_tips()` to switch on/off this section.",
      "text/html": "<pre style='margin: 1em 0 1em 0;'>Model Details\n=============\nH2OStackedEnsembleEstimator : Stacked Ensemble\nModel Key: StackedEnsemble_BestOfFamily_3_AutoML_1_20230224_110317\n</pre>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-24.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-24 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-24 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-24 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-24 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-24 .h2o-table th,\n#h2o-table-24 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-24 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-24\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Model Summary for Stacked Ensemble: </caption>\n    <thead><tr><th>key</th>\n<th>value</th></tr></thead>\n    <tbody><tr><td>Stacking strategy</td>\n<td>cross_validation</td></tr>\n<tr><td>Number of base models (used / total)</td>\n<td>6/6</td></tr>\n<tr><td># GBM base models (used / total)</td>\n<td>1/1</td></tr>\n<tr><td># XGBoost base models (used / total)</td>\n<td>1/1</td></tr>\n<tr><td># DRF base models (used / total)</td>\n<td>2/2</td></tr>\n<tr><td># GLM base models (used / total)</td>\n<td>1/1</td></tr>\n<tr><td># DeepLearning base models (used / total)</td>\n<td>1/1</td></tr>\n<tr><td>Metalearner algorithm</td>\n<td>GLM</td></tr>\n<tr><td>Metalearner fold assignment scheme</td>\n<td>Random</td></tr>\n<tr><td>Metalearner nfolds</td>\n<td>5</td></tr>\n<tr><td>Metalearner fold_column</td>\n<td>None</td></tr>\n<tr><td>Custom metalearner hyperparameters</td>\n<td>None</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.090302569991916\nRMSE: 0.3005038601947003\nLogLoss: 0.30788977805421525\nAUC: 0.9528632548955077\nAUCPR: 0.9442040059021868\nGini: 0.9057265097910154\nNull degrees of freedom: 500\nResidual degrees of freedom: 494\nNull deviance: 678.6388715052319\nResidual deviance: 308.5055576103236\nAIC: 322.5055576103236</pre>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-25.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-25 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-25 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-25 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-25 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-25 .h2o-table th,\n#h2o-table-25 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-25 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-25\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.48698351606560014</caption>\n    <thead><tr><th></th>\n<th>0</th>\n<th>1</th>\n<th>Error</th>\n<th>Rate</th></tr></thead>\n    <tbody><tr><td>0</td>\n<td>282.0</td>\n<td>13.0</td>\n<td>0.0441</td>\n<td> (13.0/295.0)</td></tr>\n<tr><td>1</td>\n<td>39.0</td>\n<td>167.0</td>\n<td>0.1893</td>\n<td> (39.0/206.0)</td></tr>\n<tr><td>Total</td>\n<td>321.0</td>\n<td>180.0</td>\n<td>0.1038</td>\n<td> (52.0/501.0)</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-26.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-26 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-26 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-26 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-26 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-26 .h2o-table th,\n#h2o-table-26 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-26 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-26\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n    <thead><tr><th>metric</th>\n<th>threshold</th>\n<th>value</th>\n<th>idx</th></tr></thead>\n    <tbody><tr><td>max f1</td>\n<td>0.4869835</td>\n<td>0.8652850</td>\n<td>155.0</td></tr>\n<tr><td>max f2</td>\n<td>0.2601193</td>\n<td>0.8998145</td>\n<td>220.0</td></tr>\n<tr><td>max f0point5</td>\n<td>0.5490708</td>\n<td>0.9111617</td>\n<td>145.0</td></tr>\n<tr><td>max accuracy</td>\n<td>0.4869835</td>\n<td>0.8962076</td>\n<td>155.0</td></tr>\n<tr><td>max precision</td>\n<td>0.9846315</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max recall</td>\n<td>0.1130783</td>\n<td>1.0</td>\n<td>320.0</td></tr>\n<tr><td>max specificity</td>\n<td>0.9846315</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max absolute_mcc</td>\n<td>0.4869835</td>\n<td>0.7861970</td>\n<td>155.0</td></tr>\n<tr><td>max min_per_class_accuracy</td>\n<td>0.3415439</td>\n<td>0.8640777</td>\n<td>190.0</td></tr>\n<tr><td>max mean_per_class_accuracy</td>\n<td>0.4869835</td>\n<td>0.8833059</td>\n<td>155.0</td></tr>\n<tr><td>max tns</td>\n<td>0.9846315</td>\n<td>295.0</td>\n<td>0.0</td></tr>\n<tr><td>max fns</td>\n<td>0.9846315</td>\n<td>205.0</td>\n<td>0.0</td></tr>\n<tr><td>max fps</td>\n<td>0.0271721</td>\n<td>295.0</td>\n<td>399.0</td></tr>\n<tr><td>max tps</td>\n<td>0.1130783</td>\n<td>206.0</td>\n<td>320.0</td></tr>\n<tr><td>max tnr</td>\n<td>0.9846315</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max fnr</td>\n<td>0.9846315</td>\n<td>0.9951456</td>\n<td>0.0</td></tr>\n<tr><td>max fpr</td>\n<td>0.0271721</td>\n<td>1.0</td>\n<td>399.0</td></tr>\n<tr><td>max tpr</td>\n<td>0.1130783</td>\n<td>1.0</td>\n<td>320.0</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-27.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-27 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-27 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-27 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-27 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-27 .h2o-table th,\n#h2o-table-27 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-27 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-27\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Gains/Lift Table: Avg response rate: 41.12 %, avg score: 40.55 %</caption>\n    <thead><tr><th>group</th>\n<th>cumulative_data_fraction</th>\n<th>lower_threshold</th>\n<th>lift</th>\n<th>cumulative_lift</th>\n<th>response_rate</th>\n<th>score</th>\n<th>cumulative_response_rate</th>\n<th>cumulative_score</th>\n<th>capture_rate</th>\n<th>cumulative_capture_rate</th>\n<th>gain</th>\n<th>cumulative_gain</th>\n<th>kolmogorov_smirnov</th></tr></thead>\n    <tbody><tr><td>1</td>\n<td>0.0119760</td>\n<td>0.9787792</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9806054</td>\n<td>1.0</td>\n<td>0.9806054</td>\n<td>0.0291262</td>\n<td>0.0291262</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0291262</td></tr>\n<tr><td>2</td>\n<td>0.0219561</td>\n<td>0.9725229</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9760010</td>\n<td>1.0</td>\n<td>0.9785125</td>\n<td>0.0242718</td>\n<td>0.0533981</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0533981</td></tr>\n<tr><td>3</td>\n<td>0.0319361</td>\n<td>0.9699859</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9708564</td>\n<td>1.0</td>\n<td>0.9761200</td>\n<td>0.0242718</td>\n<td>0.0776699</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0776699</td></tr>\n<tr><td>4</td>\n<td>0.0419162</td>\n<td>0.9682736</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9688142</td>\n<td>1.0</td>\n<td>0.9743805</td>\n<td>0.0242718</td>\n<td>0.1019417</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.1019417</td></tr>\n<tr><td>5</td>\n<td>0.0518962</td>\n<td>0.9577170</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9635359</td>\n<td>1.0</td>\n<td>0.9722950</td>\n<td>0.0242718</td>\n<td>0.1262136</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.1262136</td></tr>\n<tr><td>6</td>\n<td>0.1017964</td>\n<td>0.9211650</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9447383</td>\n<td>1.0</td>\n<td>0.9587868</td>\n<td>0.1213592</td>\n<td>0.2475728</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.2475728</td></tr>\n<tr><td>7</td>\n<td>0.1516966</td>\n<td>0.8827008</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9030155</td>\n<td>1.0</td>\n<td>0.9404410</td>\n<td>0.1213592</td>\n<td>0.3689320</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.3689320</td></tr>\n<tr><td>8</td>\n<td>0.2015968</td>\n<td>0.8208444</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.8596583</td>\n<td>1.0</td>\n<td>0.9204453</td>\n<td>0.1213592</td>\n<td>0.4902913</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.4902913</td></tr>\n<tr><td>9</td>\n<td>0.3013972</td>\n<td>0.6492357</td>\n<td>2.1401942</td>\n<td>2.3354015</td>\n<td>0.88</td>\n<td>0.7207865</td>\n<td>0.9602649</td>\n<td>0.8543331</td>\n<td>0.2135922</td>\n<td>0.7038835</td>\n<td>114.0194175</td>\n<td>133.5401530</td>\n<td>0.6835445</td></tr>\n<tr><td>10</td>\n<td>0.4011976</td>\n<td>0.4075139</td>\n<td>1.2160194</td>\n<td>2.0569483</td>\n<td>0.5</td>\n<td>0.5133141</td>\n<td>0.8457711</td>\n<td>0.7695025</td>\n<td>0.1213592</td>\n<td>0.8252427</td>\n<td>21.6019417</td>\n<td>105.6948268</td>\n<td>0.7201580</td></tr>\n<tr><td>11</td>\n<td>0.5009980</td>\n<td>0.2609198</td>\n<td>1.0700971</td>\n<td>1.8603644</td>\n<td>0.44</td>\n<td>0.3220798</td>\n<td>0.7649402</td>\n<td>0.6803745</td>\n<td>0.1067961</td>\n<td>0.9320388</td>\n<td>7.0097087</td>\n<td>86.0364368</td>\n<td>0.7320388</td></tr>\n<tr><td>12</td>\n<td>0.6007984</td>\n<td>0.1818721</td>\n<td>0.3404854</td>\n<td>1.6078928</td>\n<td>0.14</td>\n<td>0.2198483</td>\n<td>0.6611296</td>\n<td>0.6038751</td>\n<td>0.0339806</td>\n<td>0.9660194</td>\n<td>-65.9514563</td>\n<td>60.7892785</td>\n<td>0.6202567</td></tr>\n<tr><td>13</td>\n<td>0.7005988</td>\n<td>0.1342557</td>\n<td>0.2432039</td>\n<td>1.4134927</td>\n<td>0.1</td>\n<td>0.1600129</td>\n<td>0.5811966</td>\n<td>0.5406469</td>\n<td>0.0242718</td>\n<td>0.9902913</td>\n<td>-75.6796117</td>\n<td>41.3492656</td>\n<td>0.4919862</td></tr>\n<tr><td>14</td>\n<td>0.8003992</td>\n<td>0.1084087</td>\n<td>0.0972816</td>\n<td>1.2493766</td>\n<td>0.04</td>\n<td>0.1209252</td>\n<td>0.5137157</td>\n<td>0.4883125</td>\n<td>0.0097087</td>\n<td>1.0</td>\n<td>-90.2718447</td>\n<td>24.9376559</td>\n<td>0.3389831</td></tr>\n<tr><td>15</td>\n<td>0.9001996</td>\n<td>0.0748283</td>\n<td>0.0</td>\n<td>1.1108647</td>\n<td>0.0</td>\n<td>0.0924047</td>\n<td>0.4567627</td>\n<td>0.4444203</td>\n<td>0.0</td>\n<td>1.0</td>\n<td>-100.0</td>\n<td>11.0864745</td>\n<td>0.1694915</td></tr>\n<tr><td>16</td>\n<td>1.0</td>\n<td>0.0271721</td>\n<td>0.0</td>\n<td>1.0</td>\n<td>0.0</td>\n<td>0.0539791</td>\n<td>0.4111776</td>\n<td>0.4054541</td>\n<td>0.0</td>\n<td>1.0</td>\n<td>-100.0</td>\n<td>0.0</td>\n<td>0.0</td></tr></tbody>\n  </table>\n</div>\n</div></div>\n<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: stackedensemble\n** Reported on validation data. **\n\nMSE: 0.10790103374848198\nRMSE: 0.3284829276362502\nLogLoss: 0.3641385099423577\nAUC: 0.9066458687842692\nAUCPR: 0.8922966766719541\nGini: 0.8132917375685385\nNull degrees of freedom: 210\nResidual degrees of freedom: 204\nNull deviance: 282.39644104064814\nResidual deviance: 153.66645119567494\nAIC: 167.66645119567494</pre>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-28.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-28 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-28 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-28 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-28 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-28 .h2o-table th,\n#h2o-table-28 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-28 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-28\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5038318992954588</caption>\n    <thead><tr><th></th>\n<th>0</th>\n<th>1</th>\n<th>Error</th>\n<th>Rate</th></tr></thead>\n    <tbody><tr><td>0</td>\n<td>117.0</td>\n<td>12.0</td>\n<td>0.093</td>\n<td> (12.0/129.0)</td></tr>\n<tr><td>1</td>\n<td>15.0</td>\n<td>67.0</td>\n<td>0.1829</td>\n<td> (15.0/82.0)</td></tr>\n<tr><td>Total</td>\n<td>132.0</td>\n<td>79.0</td>\n<td>0.128</td>\n<td> (27.0/211.0)</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-29.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-29 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-29 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-29 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-29 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-29 .h2o-table th,\n#h2o-table-29 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-29 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-29\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n    <thead><tr><th>metric</th>\n<th>threshold</th>\n<th>value</th>\n<th>idx</th></tr></thead>\n    <tbody><tr><td>max f1</td>\n<td>0.5038319</td>\n<td>0.8322981</td>\n<td>75.0</td></tr>\n<tr><td>max f2</td>\n<td>0.3102633</td>\n<td>0.8782201</td>\n<td>95.0</td></tr>\n<tr><td>max f0point5</td>\n<td>0.5831289</td>\n<td>0.8615819</td>\n<td>65.0</td></tr>\n<tr><td>max accuracy</td>\n<td>0.5255172</td>\n<td>0.8720379</td>\n<td>71.0</td></tr>\n<tr><td>max precision</td>\n<td>0.9788934</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max recall</td>\n<td>0.0593606</td>\n<td>1.0</td>\n<td>190.0</td></tr>\n<tr><td>max specificity</td>\n<td>0.9788934</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max absolute_mcc</td>\n<td>0.5038319</td>\n<td>0.7292388</td>\n<td>75.0</td></tr>\n<tr><td>max min_per_class_accuracy</td>\n<td>0.4034351</td>\n<td>0.8449612</td>\n<td>86.0</td></tr>\n<tr><td>max mean_per_class_accuracy</td>\n<td>0.3182856</td>\n<td>0.8659482</td>\n<td>92.0</td></tr>\n<tr><td>max tns</td>\n<td>0.9788934</td>\n<td>129.0</td>\n<td>0.0</td></tr>\n<tr><td>max fns</td>\n<td>0.9788934</td>\n<td>81.0</td>\n<td>0.0</td></tr>\n<tr><td>max fps</td>\n<td>0.0269644</td>\n<td>129.0</td>\n<td>204.0</td></tr>\n<tr><td>max tps</td>\n<td>0.0593606</td>\n<td>82.0</td>\n<td>190.0</td></tr>\n<tr><td>max tnr</td>\n<td>0.9788934</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max fnr</td>\n<td>0.9788934</td>\n<td>0.9878049</td>\n<td>0.0</td></tr>\n<tr><td>max fpr</td>\n<td>0.0269644</td>\n<td>1.0</td>\n<td>204.0</td></tr>\n<tr><td>max tpr</td>\n<td>0.0593606</td>\n<td>1.0</td>\n<td>190.0</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-30.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-30 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-30 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-30 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-30 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-30 .h2o-table th,\n#h2o-table-30 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-30 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-30\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Gains/Lift Table: Avg response rate: 38.86 %, avg score: 40.50 %</caption>\n    <thead><tr><th>group</th>\n<th>cumulative_data_fraction</th>\n<th>lower_threshold</th>\n<th>lift</th>\n<th>cumulative_lift</th>\n<th>response_rate</th>\n<th>score</th>\n<th>cumulative_response_rate</th>\n<th>cumulative_score</th>\n<th>capture_rate</th>\n<th>cumulative_capture_rate</th>\n<th>gain</th>\n<th>cumulative_gain</th>\n<th>kolmogorov_smirnov</th></tr></thead>\n    <tbody><tr><td>1</td>\n<td>0.0142180</td>\n<td>0.9729723</td>\n<td>2.5731707</td>\n<td>2.5731707</td>\n<td>1.0</td>\n<td>0.9767317</td>\n<td>1.0</td>\n<td>0.9767317</td>\n<td>0.0365854</td>\n<td>0.0365854</td>\n<td>157.3170732</td>\n<td>157.3170732</td>\n<td>0.0365854</td></tr>\n<tr><td>2</td>\n<td>0.0236967</td>\n<td>0.9583031</td>\n<td>2.5731707</td>\n<td>2.5731707</td>\n<td>1.0</td>\n<td>0.9631331</td>\n<td>1.0</td>\n<td>0.9712923</td>\n<td>0.0243902</td>\n<td>0.0609756</td>\n<td>157.3170732</td>\n<td>157.3170732</td>\n<td>0.0609756</td></tr>\n<tr><td>3</td>\n<td>0.0331754</td>\n<td>0.9562840</td>\n<td>2.5731707</td>\n<td>2.5731707</td>\n<td>1.0</td>\n<td>0.9565058</td>\n<td>1.0</td>\n<td>0.9670676</td>\n<td>0.0243902</td>\n<td>0.0853659</td>\n<td>157.3170732</td>\n<td>157.3170732</td>\n<td>0.0853659</td></tr>\n<tr><td>4</td>\n<td>0.0426540</td>\n<td>0.9540521</td>\n<td>2.5731707</td>\n<td>2.5731707</td>\n<td>1.0</td>\n<td>0.9557039</td>\n<td>1.0</td>\n<td>0.9645423</td>\n<td>0.0243902</td>\n<td>0.1097561</td>\n<td>157.3170732</td>\n<td>157.3170732</td>\n<td>0.1097561</td></tr>\n<tr><td>5</td>\n<td>0.0521327</td>\n<td>0.9501502</td>\n<td>2.5731707</td>\n<td>2.5731707</td>\n<td>1.0</td>\n<td>0.9513399</td>\n<td>1.0</td>\n<td>0.9621419</td>\n<td>0.0243902</td>\n<td>0.1341463</td>\n<td>157.3170732</td>\n<td>157.3170732</td>\n<td>0.1341463</td></tr>\n<tr><td>6</td>\n<td>0.1042654</td>\n<td>0.9263479</td>\n<td>2.3392461</td>\n<td>2.4562084</td>\n<td>0.9090909</td>\n<td>0.9391181</td>\n<td>0.9545455</td>\n<td>0.9506300</td>\n<td>0.1219512</td>\n<td>0.2560976</td>\n<td>133.9246120</td>\n<td>145.6208426</td>\n<td>0.2483456</td></tr>\n<tr><td>7</td>\n<td>0.1516588</td>\n<td>0.8954393</td>\n<td>2.5731707</td>\n<td>2.4927591</td>\n<td>1.0</td>\n<td>0.9123990</td>\n<td>0.96875</td>\n<td>0.9386828</td>\n<td>0.1219512</td>\n<td>0.3780488</td>\n<td>157.3170732</td>\n<td>149.2759146</td>\n<td>0.3702968</td></tr>\n<tr><td>8</td>\n<td>0.2037915</td>\n<td>0.7389430</td>\n<td>2.3392461</td>\n<td>2.4534884</td>\n<td>0.9090909</td>\n<td>0.8539613</td>\n<td>0.9534884</td>\n<td>0.9170099</td>\n<td>0.1219512</td>\n<td>0.5</td>\n<td>133.9246120</td>\n<td>145.3488372</td>\n<td>0.4844961</td></tr>\n<tr><td>9</td>\n<td>0.3033175</td>\n<td>0.6173731</td>\n<td>1.9605110</td>\n<td>2.2917302</td>\n<td>0.7619048</td>\n<td>0.6897821</td>\n<td>0.890625</td>\n<td>0.8424508</td>\n<td>0.1951220</td>\n<td>0.6951220</td>\n<td>96.0511034</td>\n<td>129.1730183</td>\n<td>0.6408584</td></tr>\n<tr><td>10</td>\n<td>0.4028436</td>\n<td>0.4281485</td>\n<td>1.3478513</td>\n<td>2.0585366</td>\n<td>0.5238095</td>\n<td>0.5262224</td>\n<td>0.8</td>\n<td>0.7643237</td>\n<td>0.1341463</td>\n<td>0.8292683</td>\n<td>34.7851336</td>\n<td>105.8536585</td>\n<td>0.6974853</td></tr>\n<tr><td>11</td>\n<td>0.5023697</td>\n<td>0.2558777</td>\n<td>0.8577236</td>\n<td>1.8206397</td>\n<td>0.3333333</td>\n<td>0.3357459</td>\n<td>0.7075472</td>\n<td>0.6794168</td>\n<td>0.0853659</td>\n<td>0.9146341</td>\n<td>-14.2276423</td>\n<td>82.0639669</td>\n<td>0.6743241</td></tr>\n<tr><td>12</td>\n<td>0.6018957</td>\n<td>0.1868997</td>\n<td>0.1225319</td>\n<td>1.5398502</td>\n<td>0.0476190</td>\n<td>0.2207721</td>\n<td>0.5984252</td>\n<td>0.6035779</td>\n<td>0.0121951</td>\n<td>0.9268293</td>\n<td>-87.7468060</td>\n<td>53.9850202</td>\n<td>0.5314804</td></tr>\n<tr><td>13</td>\n<td>0.7014218</td>\n<td>0.1320844</td>\n<td>0.1225319</td>\n<td>1.3387442</td>\n<td>0.0476190</td>\n<td>0.1626075</td>\n<td>0.5202703</td>\n<td>0.5410078</td>\n<td>0.0121951</td>\n<td>0.9390244</td>\n<td>-87.7468060</td>\n<td>33.8744232</td>\n<td>0.3886368</td></tr>\n<tr><td>14</td>\n<td>0.8009479</td>\n<td>0.1004387</td>\n<td>0.1225319</td>\n<td>1.1876173</td>\n<td>0.0476190</td>\n<td>0.1153699</td>\n<td>0.4615385</td>\n<td>0.4881179</td>\n<td>0.0121951</td>\n<td>0.9512195</td>\n<td>-87.7468060</td>\n<td>18.7617261</td>\n<td>0.2457932</td></tr>\n<tr><td>15</td>\n<td>0.9004739</td>\n<td>0.0727439</td>\n<td>0.2450639</td>\n<td>1.0834403</td>\n<td>0.0952381</td>\n<td>0.0858364</td>\n<td>0.4210526</td>\n<td>0.4436552</td>\n<td>0.0243902</td>\n<td>0.9756098</td>\n<td>-75.4936121</td>\n<td>8.3440308</td>\n<td>0.1228966</td></tr>\n<tr><td>16</td>\n<td>1.0</td>\n<td>0.0269644</td>\n<td>0.2450639</td>\n<td>1.0</td>\n<td>0.0952381</td>\n<td>0.0548286</td>\n<td>0.3886256</td>\n<td>0.4049568</td>\n<td>0.0243902</td>\n<td>1.0</td>\n<td>-75.4936121</td>\n<td>0.0</td>\n<td>0.0</td></tr></tbody>\n  </table>\n</div>\n</div></div>\n<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: stackedensemble\n** Reported on cross-validation data. **\n\nMSE: 0.14648277687501338\nRMSE: 0.38273068452243725\nLogLoss: 0.4593560459236398\nAUC: 0.8461329603422741\nAUCPR: 0.8292932326677792\nGini: 0.6922659206845483\nNull degrees of freedom: 500\nResidual degrees of freedom: 494\nNull deviance: 679.6192027459856\nResidual deviance: 460.2747580154872\nAIC: 474.2747580154872</pre>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-31.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-31 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-31 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-31 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-31 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-31 .h2o-table th,\n#h2o-table-31 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-31 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-31\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.42980900650465503</caption>\n    <thead><tr><th></th>\n<th>0</th>\n<th>1</th>\n<th>Error</th>\n<th>Rate</th></tr></thead>\n    <tbody><tr><td>0</td>\n<td>241.0</td>\n<td>54.0</td>\n<td>0.1831</td>\n<td> (54.0/295.0)</td></tr>\n<tr><td>1</td>\n<td>46.0</td>\n<td>160.0</td>\n<td>0.2233</td>\n<td> (46.0/206.0)</td></tr>\n<tr><td>Total</td>\n<td>287.0</td>\n<td>214.0</td>\n<td>0.1996</td>\n<td> (100.0/501.0)</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-32.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-32 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-32 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-32 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-32 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-32 .h2o-table th,\n#h2o-table-32 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-32 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-32\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n    <thead><tr><th>metric</th>\n<th>threshold</th>\n<th>value</th>\n<th>idx</th></tr></thead>\n    <tbody><tr><td>max f1</td>\n<td>0.4298090</td>\n<td>0.7619048</td>\n<td>189.0</td></tr>\n<tr><td>max f2</td>\n<td>0.1390808</td>\n<td>0.8098007</td>\n<td>322.0</td></tr>\n<tr><td>max f0point5</td>\n<td>0.7528128</td>\n<td>0.8076923</td>\n<td>91.0</td></tr>\n<tr><td>max accuracy</td>\n<td>0.5150953</td>\n<td>0.8083832</td>\n<td>156.0</td></tr>\n<tr><td>max precision</td>\n<td>0.9868134</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max recall</td>\n<td>0.0316588</td>\n<td>1.0</td>\n<td>399.0</td></tr>\n<tr><td>max specificity</td>\n<td>0.9868134</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max absolute_mcc</td>\n<td>0.5150953</td>\n<td>0.6001909</td>\n<td>156.0</td></tr>\n<tr><td>max min_per_class_accuracy</td>\n<td>0.3793733</td>\n<td>0.7864078</td>\n<td>199.0</td></tr>\n<tr><td>max mean_per_class_accuracy</td>\n<td>0.4298090</td>\n<td>0.7968241</td>\n<td>189.0</td></tr>\n<tr><td>max tns</td>\n<td>0.9868134</td>\n<td>295.0</td>\n<td>0.0</td></tr>\n<tr><td>max fns</td>\n<td>0.9868134</td>\n<td>205.0</td>\n<td>0.0</td></tr>\n<tr><td>max fps</td>\n<td>0.0354786</td>\n<td>295.0</td>\n<td>398.0</td></tr>\n<tr><td>max tps</td>\n<td>0.0316588</td>\n<td>206.0</td>\n<td>399.0</td></tr>\n<tr><td>max tnr</td>\n<td>0.9868134</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max fnr</td>\n<td>0.9868134</td>\n<td>0.9951456</td>\n<td>0.0</td></tr>\n<tr><td>max fpr</td>\n<td>0.0354786</td>\n<td>1.0</td>\n<td>398.0</td></tr>\n<tr><td>max tpr</td>\n<td>0.0316588</td>\n<td>1.0</td>\n<td>399.0</td></tr></tbody>\n  </table>\n</div>\n</div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-33.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-33 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-33 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-33 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-33 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-33 .h2o-table th,\n#h2o-table-33 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-33 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-33\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Gains/Lift Table: Avg response rate: 41.12 %, avg score: 41.11 %</caption>\n    <thead><tr><th>group</th>\n<th>cumulative_data_fraction</th>\n<th>lower_threshold</th>\n<th>lift</th>\n<th>cumulative_lift</th>\n<th>response_rate</th>\n<th>score</th>\n<th>cumulative_response_rate</th>\n<th>cumulative_score</th>\n<th>capture_rate</th>\n<th>cumulative_capture_rate</th>\n<th>gain</th>\n<th>cumulative_gain</th>\n<th>kolmogorov_smirnov</th></tr></thead>\n    <tbody><tr><td>1</td>\n<td>0.0119760</td>\n<td>0.9787940</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9823272</td>\n<td>1.0</td>\n<td>0.9823272</td>\n<td>0.0291262</td>\n<td>0.0291262</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0291262</td></tr>\n<tr><td>2</td>\n<td>0.0219561</td>\n<td>0.9639371</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9697411</td>\n<td>1.0</td>\n<td>0.9766062</td>\n<td>0.0242718</td>\n<td>0.0533981</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0533981</td></tr>\n<tr><td>3</td>\n<td>0.0319361</td>\n<td>0.9567024</td>\n<td>2.4320388</td>\n<td>2.4320388</td>\n<td>1.0</td>\n<td>0.9597904</td>\n<td>1.0</td>\n<td>0.9713513</td>\n<td>0.0242718</td>\n<td>0.0776699</td>\n<td>143.2038835</td>\n<td>143.2038835</td>\n<td>0.0776699</td></tr>\n<tr><td>4</td>\n<td>0.0419162</td>\n<td>0.9496254</td>\n<td>1.9456311</td>\n<td>2.3162275</td>\n<td>0.8</td>\n<td>0.9518798</td>\n<td>0.9523810</td>\n<td>0.9667152</td>\n<td>0.0194175</td>\n<td>0.0970874</td>\n<td>94.5631068</td>\n<td>131.6227462</td>\n<td>0.0936975</td></tr>\n<tr><td>5</td>\n<td>0.0518962</td>\n<td>0.9368296</td>\n<td>2.4320388</td>\n<td>2.3384989</td>\n<td>1.0</td>\n<td>0.9424199</td>\n<td>0.9615385</td>\n<td>0.9620430</td>\n<td>0.0242718</td>\n<td>0.1213592</td>\n<td>143.2038835</td>\n<td>133.8498880</td>\n<td>0.1179694</td></tr>\n<tr><td>6</td>\n<td>0.1017964</td>\n<td>0.9061738</td>\n<td>2.2374757</td>\n<td>2.2889777</td>\n<td>0.92</td>\n<td>0.9199909</td>\n<td>0.9411765</td>\n<td>0.9414293</td>\n<td>0.1116505</td>\n<td>0.2330097</td>\n<td>123.7475728</td>\n<td>128.8977727</td>\n<td>0.2228402</td></tr>\n<tr><td>7</td>\n<td>0.1516966</td>\n<td>0.8472418</td>\n<td>2.3347573</td>\n<td>2.3040368</td>\n<td>0.96</td>\n<td>0.8807309</td>\n<td>0.9473684</td>\n<td>0.9214627</td>\n<td>0.1165049</td>\n<td>0.3495146</td>\n<td>133.4757282</td>\n<td>130.4036791</td>\n<td>0.3359552</td></tr>\n<tr><td>8</td>\n<td>0.2015968</td>\n<td>0.7755974</td>\n<td>2.2374757</td>\n<td>2.2875613</td>\n<td>0.92</td>\n<td>0.8074830</td>\n<td>0.9405941</td>\n<td>0.8932499</td>\n<td>0.1116505</td>\n<td>0.4611650</td>\n<td>123.7475728</td>\n<td>128.7561280</td>\n<td>0.4408261</td></tr>\n<tr><td>9</td>\n<td>0.3013972</td>\n<td>0.6086286</td>\n<td>1.5078641</td>\n<td>2.0293834</td>\n<td>0.62</td>\n<td>0.6769152</td>\n<td>0.8344371</td>\n<td>0.8216159</td>\n<td>0.1504854</td>\n<td>0.6116505</td>\n<td>50.7864078</td>\n<td>102.9383399</td>\n<td>0.5269047</td></tr>\n<tr><td>10</td>\n<td>0.4011976</td>\n<td>0.4636780</td>\n<td>1.2646602</td>\n<td>1.8391537</td>\n<td>0.52</td>\n<td>0.5236000</td>\n<td>0.7562189</td>\n<td>0.7474826</td>\n<td>0.1262136</td>\n<td>0.7378641</td>\n<td>26.4660194</td>\n<td>83.9153746</td>\n<td>0.5717624</td></tr>\n<tr><td>11</td>\n<td>0.5009980</td>\n<td>0.2917765</td>\n<td>0.6323301</td>\n<td>1.5987506</td>\n<td>0.26</td>\n<td>0.3732606</td>\n<td>0.6573705</td>\n<td>0.6729364</td>\n<td>0.0631068</td>\n<td>0.8009709</td>\n<td>-36.7669903</td>\n<td>59.8750629</td>\n<td>0.5094455</td></tr>\n<tr><td>12</td>\n<td>0.6007984</td>\n<td>0.2128191</td>\n<td>0.4864078</td>\n<td>1.4139761</td>\n<td>0.2</td>\n<td>0.2420837</td>\n<td>0.5813953</td>\n<td>0.6013661</td>\n<td>0.0485437</td>\n<td>0.8495146</td>\n<td>-51.3592233</td>\n<td>41.3976067</td>\n<td>0.4223959</td></tr>\n<tr><td>13</td>\n<td>0.7005988</td>\n<td>0.1620148</td>\n<td>0.4864078</td>\n<td>1.2818438</td>\n<td>0.2</td>\n<td>0.1858014</td>\n<td>0.5270655</td>\n<td>0.5421689</td>\n<td>0.0485437</td>\n<td>0.8980583</td>\n<td>-51.3592233</td>\n<td>28.1843830</td>\n<td>0.3353464</td></tr>\n<tr><td>14</td>\n<td>0.8003992</td>\n<td>0.1247093</td>\n<td>0.4864078</td>\n<td>1.1826623</td>\n<td>0.2</td>\n<td>0.1422316</td>\n<td>0.4862843</td>\n<td>0.4923014</td>\n<td>0.0485437</td>\n<td>0.9466019</td>\n<td>-51.3592233</td>\n<td>18.2662276</td>\n<td>0.2482969</td></tr>\n<tr><td>15</td>\n<td>0.9001996</td>\n<td>0.0859383</td>\n<td>0.3404854</td>\n<td>1.0892946</td>\n<td>0.14</td>\n<td>0.1057175</td>\n<td>0.4478936</td>\n<td>0.4494429</td>\n<td>0.0339806</td>\n<td>0.9805825</td>\n<td>-65.9514563</td>\n<td>8.9294556</td>\n<td>0.1365147</td></tr>\n<tr><td>16</td>\n<td>1.0</td>\n<td>0.0316588</td>\n<td>0.1945631</td>\n<td>1.0</td>\n<td>0.08</td>\n<td>0.0655716</td>\n<td>0.4111776</td>\n<td>0.4111324</td>\n<td>0.0194175</td>\n<td>1.0</td>\n<td>-80.5436893</td>\n<td>0.0</td>\n<td>0.0</td></tr></tbody>\n  </table>\n</div>\n</div></div>\n<div style='margin: 1em 0 1em 0;'>\n<style>\n\n#h2o-table-34.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-34 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-34 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-34 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-34 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-34 .h2o-table th,\n#h2o-table-34 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-34 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-34\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption>Cross-Validation Metrics Summary: </caption>\n    <thead><tr><th></th>\n<th>mean</th>\n<th>sd</th>\n<th>cv_1_valid</th>\n<th>cv_2_valid</th>\n<th>cv_3_valid</th>\n<th>cv_4_valid</th>\n<th>cv_5_valid</th></tr></thead>\n    <tbody><tr><td>accuracy</td>\n<td>0.8231532</td>\n<td>0.0480694</td>\n<td>0.78</td>\n<td>0.8295454</td>\n<td>0.8712871</td>\n<td>0.7676768</td>\n<td>0.8672566</td></tr>\n<tr><td>auc</td>\n<td>0.8456506</td>\n<td>0.0371102</td>\n<td>0.8529167</td>\n<td>0.7946623</td>\n<td>0.8629908</td>\n<td>0.8256198</td>\n<td>0.8920635</td></tr>\n<tr><td>err</td>\n<td>0.1768468</td>\n<td>0.0480694</td>\n<td>0.22</td>\n<td>0.1704546</td>\n<td>0.1287129</td>\n<td>0.2323232</td>\n<td>0.1327434</td></tr>\n<tr><td>err_count</td>\n<td>17.6</td>\n<td>4.560702</td>\n<td>22.0</td>\n<td>15.0</td>\n<td>13.0</td>\n<td>23.0</td>\n<td>15.0</td></tr>\n<tr><td>f0point5</td>\n<td>0.796463</td>\n<td>0.069022</td>\n<td>0.7083333</td>\n<td>0.8333333</td>\n<td>0.8426966</td>\n<td>0.7366071</td>\n<td>0.8613445</td></tr>\n<tr><td>f1</td>\n<td>0.7802499</td>\n<td>0.0499139</td>\n<td>0.7555556</td>\n<td>0.7368421</td>\n<td>0.8219178</td>\n<td>0.7415730</td>\n<td>0.8453608</td></tr>\n<tr><td>f2</td>\n<td>0.7697212</td>\n<td>0.0684730</td>\n<td>0.8095238</td>\n<td>0.6603774</td>\n<td>0.8021390</td>\n<td>0.7466063</td>\n<td>0.8299595</td></tr>\n<tr><td>lift_top_group</td>\n<td>2.451226</td>\n<td>0.1876977</td>\n<td>2.5</td>\n<td>2.5882354</td>\n<td>2.6578948</td>\n<td>2.25</td>\n<td>2.26</td></tr>\n<tr><td>logloss</td>\n<td>0.4596301</td>\n<td>0.0406558</td>\n<td>0.449594</td>\n<td>0.5052692</td>\n<td>0.4246705</td>\n<td>0.4993432</td>\n<td>0.4192738</td></tr>\n<tr><td>max_per_class_error</td>\n<td>0.2579092</td>\n<td>0.0773636</td>\n<td>0.2666667</td>\n<td>0.3823530</td>\n<td>0.2105263</td>\n<td>0.25</td>\n<td>0.18</td></tr>\n<tr><td>---</td>\n<td>---</td>\n<td>---</td>\n<td>---</td>\n<td>---</td>\n<td>---</td>\n<td>---</td>\n<td>---</td></tr>\n<tr><td>mean_per_class_error</td>\n<td>0.1869368</td>\n<td>0.0429964</td>\n<td>0.2083333</td>\n<td>0.209695</td>\n<td>0.1449457</td>\n<td>0.2340909</td>\n<td>0.1376190</td></tr>\n<tr><td>mse</td>\n<td>0.1469052</td>\n<td>0.0170543</td>\n<td>0.1461015</td>\n<td>0.1655278</td>\n<td>0.1329034</td>\n<td>0.1624982</td>\n<td>0.1274950</td></tr>\n<tr><td>null_deviance</td>\n<td>135.92384</td>\n<td>13.548579</td>\n<td>134.683</td>\n<td>117.73908</td>\n<td>134.56567</td>\n<td>136.71875</td>\n<td>155.9127</td></tr>\n<tr><td>pr_auc</td>\n<td>0.8313398</td>\n<td>0.0278998</td>\n<td>0.8331954</td>\n<td>0.7892128</td>\n<td>0.8502845</td>\n<td>0.8226492</td>\n<td>0.8613572</td></tr>\n<tr><td>precision</td>\n<td>0.811172</td>\n<td>0.0993701</td>\n<td>0.68</td>\n<td>0.9130435</td>\n<td>0.8571429</td>\n<td>0.7333333</td>\n<td>0.8723404</td></tr>\n<tr><td>r2</td>\n<td>0.3903643</td>\n<td>0.0719063</td>\n<td>0.3912436</td>\n<td>0.301826</td>\n<td>0.4336894</td>\n<td>0.3418824</td>\n<td>0.4831799</td></tr>\n<tr><td>recall</td>\n<td>0.7654241</td>\n<td>0.0905345</td>\n<td>0.85</td>\n<td>0.6176470</td>\n<td>0.7894737</td>\n<td>0.75</td>\n<td>0.82</td></tr>\n<tr><td>residual_deviance</td>\n<td>91.651085</td>\n<td>5.1623745</td>\n<td>89.9188</td>\n<td>88.927376</td>\n<td>85.78343</td>\n<td>98.86994</td>\n<td>94.75588</td></tr>\n<tr><td>rmse</td>\n<td>0.3827635</td>\n<td>0.0222842</td>\n<td>0.3822323</td>\n<td>0.4068511</td>\n<td>0.3645592</td>\n<td>0.4031106</td>\n<td>0.3570644</td></tr>\n<tr><td>specificity</td>\n<td>0.8607023</td>\n<td>0.0980255</td>\n<td>0.7333333</td>\n<td>0.962963</td>\n<td>0.9206349</td>\n<td>0.7818182</td>\n<td>0.9047619</td></tr></tbody>\n  </table>\n</div>\n<pre style='font-size: smaller; margin-bottom: 1em;'>[22 rows x 8 columns]</pre></div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n\n[tips]\nUse `model.explain()` to inspect the model.\n--\nUse `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o.load_model(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
