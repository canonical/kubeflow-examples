{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c793632b",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "# ML Workflow Demo: Kubeflow - Katib - MLflow\n",
    "\n",
    "## Overview\n",
    "\n",
    "This guide intended to introduce end users to complete a ML workflow using Kubeflow. In particular, examples of Kubeflow pipelines using Katib hyperparameter tuning and MLflow model registry are presented along with some common pipeline steps and interfaces such as S3.\n",
    "\n",
    "The following diagram outlines ML workflow presented in this guide. Major pipeline steps include:\n",
    "- Ingestion of dataset.\n",
    "- Cleaning up the dataset.\n",
    "- Store of cleaned data to S3 bucket.\n",
    "- Hyperparameter tuning using Katib and Tensorflow training container image (with MLflow store functionality).\n",
    "- Converting Katib results to streamlined format.\n",
    "- Model training using best parameters from tuning.\n",
    "- Storing the resulting production model to MLflow model registry.\n",
    "\n",
    "![Diagram](/images/ML-Workflow-Demo-Diagram.png)\n",
    "\n",
    "This repository contains all artifacts needed to support this guide. `images/` directory contains all related screenshorts and diagrams. `resources/` directory contains Jupyter notebook containing all steps in this guide, `Dockerfile` and Python script for training image used in this guide.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Deployed Kubeflow instance including Katib, and access to Kubeflow dashboard. For sample Kubeflow deployment refer to https://charmed-kubeflow.io/docs/quickstart (note: the `kubeflow-lite` bundle does not include Katib - `juju deploy kubeflow --trust` instead when you get to that step)\n",
    "- Deployed MLflow. For deployment of Charmed MLflow refer to https://charmed-kubeflow.io/docs/mlflow\n",
    "- Familiarity with Python, Docker, Jupyter notebooks.\n",
    "\n",
    "## Instructions\n",
    "The following are the instructions that outline the workflow process.\n",
    "\n",
    "1. Access Kubeflow dashboard via URL, eg http://10.64.140.43.nip.io/\n",
    "\n",
    "2. Navigate to Notebooks.\n",
    "\n",
    "3. Create a new notebook.\n",
    "  a. Fill in name\n",
    "  b. Select Tensorflow image `jupyter-tensorflow-full:v1.6.0`\n",
    "  c. Select minimum configuration: 1 CPU and 4GB of RAM\n",
    "\n",
    "![NotebookCreate](./images/ML-Workflow-NotebookCreate-diag.png)\n",
    "\n",
    "![NewNotebook](./images/ML-Workflow-NewNotebook-diag.png)\n",
    "\n",
    "4. Connect to the newly created notebook.\n",
    "\n",
    "5. Create a Jupyter notebook to hold code that will specify the Kubeflow pipeline.\n",
    "\n",
    "![NewJupyterNotebook](./images/ML-Workflow-NewJupyterNotebook-diag.png)\n",
    "\n",
    "NOTE: The following Jupyter notebook contains all the steps outlined below: [ml-workflow-demo-kfp-katib-mlflow.ipynb](./resources/ml-workflow-demo-kfp-katib-mlflow.ipynb)\n",
    "\n",
    "6. To setup environment add the following cells to the newly created Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ac9f9-3a58-4b56-867a-d84c88de61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp==1.8.12\n",
    "!pip install kubeflow-katib==0.13.0\n",
    "\n",
    "import json\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import Client\n",
    "from kfp import components\n",
    "from kfp.onprem import use_k8s_secret\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from kubeflow.katib import ApiClient\n",
    "from kubeflow.katib import V1beta1ExperimentSpec\n",
    "from kubeflow.katib import V1beta1AlgorithmSpec\n",
    "from kubeflow.katib import V1beta1ObjectiveSpec\n",
    "from kubeflow.katib import V1beta1ParameterSpec\n",
    "from kubeflow.katib import V1beta1FeasibleSpace\n",
    "from kubeflow.katib import V1beta1TrialTemplate\n",
    "from kubeflow.katib import V1beta1TrialParameterSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ae698",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "7. Create pipeline steps that will do data ingestion and cleanup. Setup transfer of clean data to the next step using S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0e285-6158-4eec-a6b4-7ffcf23b4000",
   "metadata": {},
   "source": [
    "Cleaning our data is a process really specific to our problem.  We know what we want to do and can do it in Python, but we need to get that logic into a step in our Kubeflow Pipeline.  Full documentation on the different ways to write and reuse Pipeline steps is [in the upstream docs](), but we include a few examples below. \n",
    "\n",
    "One way to do this is called a lightweight pipeline step, where we write a self-contained python function that does what we want and Kubeflow Pipelines helps package it for us.  For example, we can write:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21278f09-7938-419e-a9d0-d57ad3952b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data clean up operation.\n",
    "def clean_arff_data(\n",
    "    bucket,\n",
    "    key,\n",
    "    input_file: components.InputPath(str)\n",
    "):\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    import os\n",
    "    from io import StringIO\n",
    "    from scipy.io.arff import loadarff\n",
    "\n",
    "    print(f\"Loading input file {input_file}\")\n",
    "\n",
    "    # Convert to dataframe arff format.\n",
    "    raw_data = loadarff(input_file)\n",
    "    df_data = pd.DataFrame(raw_data[0].copy())\n",
    "    print(f\"Loaded data file of shape {df_data.shape}\")\n",
    "\n",
    "    print(f\"Cleaning the data\")\n",
    "    \n",
    "    # Convert target column to numeric.\n",
    "    df_data.iloc[:, -1] = pd.get_dummies(df_data['CHURN']).iloc[:, 0]\n",
    "\n",
    "    # Remove missing values.\n",
    "    df_clean = df_data.dropna(axis=1)\n",
    "    df_clean.loc[:,'CHURN'] = pd.get_dummies(df_data['CHURN']).iloc[:, 0]\n",
    "\n",
    "    # Get rid of non-numeric columns.\n",
    "    df_clean = df_clean.select_dtypes(exclude='object')\n",
    "\n",
    "    print(f\"Saving the cleaned data to S3\")\n",
    "    csv_buffer = StringIO()\n",
    "    df_clean.to_csv(csv_buffer)\n",
    "    s3_resource = boto3.resource(\n",
    "        's3',\n",
    "        endpoint_url='http://minio.kubeflow.svc.cluster.local:9000',\n",
    "        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    )\n",
    "    check_bucket = s3_resource.Bucket(bucket)\n",
    "    if not check_bucket.creation_date:\n",
    "        # bucket does not exist, create it\n",
    "        s3_resource.create_bucket(Bucket=bucket)\n",
    "    print(f\"Saving CSV of shape {df_clean.shape} to s3\")\n",
    "    s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd26b9-c8b3-4b12-ba9b-1a7bacb7f646",
   "metadata": {},
   "source": [
    "which:\n",
    "* receives the local path to a data file as an argument and loads that file (how this file **is** local is discussed later)\n",
    "* does some data cleaning (trivial in our case, but could be complex)\n",
    "* saves the cleaned data to a location in S3 specified by arguments (note that the S3 url is hard-coded here, but it could also be an argument if it needed to change)\n",
    "\n",
    "Note too that we include and `import` statements needed by our function *inside* the function definition.  Normally this is bad practice, but it is a requirement for these lightweight pipeline steps because Kubeflow Pipelines only knows about what happens inside the function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf74e86-2363-48d2-ac58-d8f8a0c6f057",
   "metadata": {},
   "source": [
    "Now that we have a python function that can do our cleaning, we need to package it into a Kubeflow Pipelines Component.  Components are reusable descriptions of how to create a step in a pipeline, represented by YAML files.  Conveniently, the kfp SDK provides us with tooling to create a component from a regular python function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d0d35-fe87-4847-baf8-44459113f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_op = components.create_component_from_func(\n",
    "    clean_arff_data,\n",
    "    output_component_file=\"clean_data.yaml\",\n",
    "    packages_to_install=[\"pandas==1.2.4\", \"scipy==1.7.0\", \"boto3\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d1838-6fea-4861-b06c-365d1182288b",
   "metadata": {},
   "source": [
    "This command generates the file `clean_data.yaml` - go check it out!  You'll see it wrapped our python function into a `python -c` command line call with a little package and argument management around it.  This is why we needed our `import` statements *inside* the function, KFP only knows about the code inside the function it is compiling into a component.  This is all executed it in a basic `python:3.X` docker image (see the `image: python:3.7` line in the spec).  Because the basic python image doesn't have the dependencies we needed, we set `packages_to_install` to define which extra dependencies should be intalled before script execution.\n",
    "\n",
    "While `clean_data.yaml` describes what a clean_data component looks like, the `clean_data_op` returned here is a factory class to actually *create* pipeline steps from the component specification.  It is what we will use later in our pipeline to actually *do* clean_data operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fe3123-137c-4deb-912a-7d6340bfa974",
   "metadata": {},
   "source": [
    "To actually fetch raw data, we can download it from the internet.  While we could write a python function to do this like above, getting files from the internet is so common that Kubeflow Pipelines provides us with a [reusable web downloader component](https://github.com/kubeflow/pipelines/blob/master/components/contrib/web/Download/component.yaml) that we can use here.  \n",
    "\n",
    "To do this, we simply load this component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64bcf41-4e30-4f2d-9ea8-a0bfdfd3b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingest operation.\n",
    "# Output is in outputs['data']\n",
    "ingest_data_op = components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/contrib/web/Download/component.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24208ef",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "8. Create the next pipeline step that will do hyperparameter tuning using Katib and a training container image `docker.io/misohu/kubeflow-training:latest`. For more details on the training container image refer to [resources README](./resources/README.md) of this guide.  Note too that this step is a bit complex, but don't worry too much if you don't understand it all right way.  In essense, this just sets up a pipeline step that does Katib tuning using a [reusable Katib launcher component](https://github.com/kubeflow/pipelines/blob/master/components/kubeflow/katib-launcher/component.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397ddc8-8e6f-438a-a014-bbd45aaed5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Katib hyperparameter tuning operation.\n",
    "def create_katib_experiment_op(experiment_name, experiment_namespace, bucket, key):\n",
    "    # Trial count specification.\n",
    "    max_trial_count = 5\n",
    "    max_failed_trial_count = 3\n",
    "    parallel_trial_count = 2\n",
    "\n",
    "    # Objective specification.\n",
    "    objective = V1beta1ObjectiveSpec(\n",
    "        type=\"maximize\",\n",
    "        goal=0.95,\n",
    "        objective_metric_name=\"accuracy\"\n",
    "    )\n",
    "\n",
    "    # Algorithm specification.\n",
    "    algorithm = V1beta1AlgorithmSpec(\n",
    "        algorithm_name=\"random\",\n",
    "    )\n",
    "\n",
    "    # Experiment search space.\n",
    "    # In this example we tune the number of epochs.\n",
    "    parameters = [\n",
    "        V1beta1ParameterSpec(\n",
    "            name=\"epochs\",\n",
    "            parameter_type=\"int\",\n",
    "            feasible_space=V1beta1FeasibleSpace(\n",
    "                min=\"5\",\n",
    "                max=\"10\"\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Experiment trial template.\n",
    "    trial_spec = {\n",
    "        \"apiVersion\": \"kubeflow.org/v1\",\n",
    "        \"kind\": \"TFJob\",\n",
    "        \"spec\": {\n",
    "            \"tfReplicaSpecs\": {\n",
    "                \"Chief\": {\n",
    "                    \"replicas\": 1,\n",
    "                    \"restartPolicy\": \"OnFailure\",\n",
    "                    \"template\": {\n",
    "                        \"metadata\": {\n",
    "                            \"annotations\": {\n",
    "                                \"sidecar.istio.io/inject\": \"false\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": \"tensorflow\",\n",
    "                                    \"image\": \"docker.io/misohu/kubeflow-training:latest\",\n",
    "                                    \"command\": [\n",
    "                                        \"python\",\n",
    "                                        \"/opt/model.py\",\n",
    "                                        \"--s3-storage=true\",\n",
    "                                        \"--epochs=${trialParameters.epochs}\",\n",
    "                                        f\"--bucket={bucket}\",\n",
    "                                        f\"--bucket-key={key}\",\n",
    "                                    ],\n",
    "                                    \"envFrom\": [\n",
    "                                      {\n",
    "                                        \"secretRef\": {\n",
    "                                          \"name\": \"mlpipeline-minio-artifact\"\n",
    "                                        }\n",
    "                                      }\n",
    "                                    ]\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "               \"Worker\": {\n",
    "                    \"replicas\": 1,\n",
    "                    \"restartPolicy\": \"OnFailure\",\n",
    "                    \"template\": {\n",
    "                        \"metadata\": {\n",
    "                            \"annotations\": {\n",
    "                                \"sidecar.istio.io/inject\": \"false\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": \"tensorflow\",\n",
    "                                    \"image\": \"docker.io/misohu/kubeflow-training:latest\",\n",
    "                                    \"command\": [\n",
    "                                        \"python\",\n",
    "                                        \"/opt/model.py\",\n",
    "                                        f\"--s3-storage=true\",\n",
    "                                        \"--epochs=${trialParameters.epochs}\",\n",
    "                                        f\"--bucket={bucket}\",\n",
    "                                        f\"--bucket-key={key}\",\n",
    "                                    ],\n",
    "                                    \"envFrom\": [\n",
    "                                      {\n",
    "                                        \"secretRef\": {\n",
    "                                          \"name\": \"mlpipeline-minio-artifact\"\n",
    "                                        }\n",
    "                                      }\n",
    "                                    ]\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Configure parameters for the Trial template.\n",
    "    trial_template = V1beta1TrialTemplate(\n",
    "        primary_container_name=\"tensorflow\",\n",
    "        trial_parameters=[\n",
    "            V1beta1TrialParameterSpec(\n",
    "                name=\"epochs\",\n",
    "                description=\"Learning rate for the training model\",\n",
    "                reference=\"epochs\"\n",
    "            )\n",
    "        ],\n",
    "        trial_spec=trial_spec\n",
    "    )\n",
    "\n",
    "    # Create an Experiment from the above parameters.\n",
    "    experiment_spec = V1beta1ExperimentSpec(\n",
    "        max_trial_count=max_trial_count,\n",
    "        max_failed_trial_count=max_failed_trial_count,\n",
    "        parallel_trial_count=parallel_trial_count,\n",
    "        objective=objective,\n",
    "        algorithm=algorithm,\n",
    "        parameters=parameters,\n",
    "        trial_template=trial_template\n",
    "    )\n",
    "\n",
    "    # Create the KFP operation for the Katib experiment.\n",
    "    # Experiment spec should be serialized to a valid Kubernetes object.\n",
    "    katib_experiment_launcher_op = components.load_component_from_url(\n",
    "        \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/katib-launcher/component.yaml\")\n",
    "    op = katib_experiment_launcher_op(\n",
    "        experiment_name=experiment_name,\n",
    "        experiment_namespace=experiment_namespace,\n",
    "        experiment_spec=ApiClient().sanitize_for_serialization(experiment_spec),\n",
    "        experiment_timeout_minutes=60,\n",
    "        delete_finished_experiment=False)\n",
    "\n",
    "    return op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042c760-a279-4da0-ac15-4e88749e843f",
   "metadata": {},
   "source": [
    "We also need to massage the Katib outputs a bit, so we add another lightweight pipeline step like we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3af79b-d70f-4703-b945-a4fd1cd05e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Katib experiment hyperparameter results to arguments in string format.\n",
    "def convert_katib_results(katib_results) -> str:\n",
    "    import json\n",
    "    import pprint\n",
    "    katib_results_json = json.loads(katib_results)\n",
    "    print(\"Katib hyperparameter tuning results:\")\n",
    "    pprint.pprint(katib_results_json)\n",
    "    best_hps = []\n",
    "    for pa in katib_results_json[\"currentOptimalTrial\"][\"parameterAssignments\"]:\n",
    "        if pa[\"name\"] == \"epochs\":\n",
    "            best_hps.append(\"--epochs=\" + pa[\"value\"])\n",
    "    print(\"Best hyperparameters: {}\".format(best_hps))\n",
    "    return \" \".join(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26862b5a-277f-4f1e-adf5-6133b1e1dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_katib_results_op = components.func_to_container_op(convert_katib_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0961a5d",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "9. Create the last step of the pipeline that will do model training using Tensorflow based on Katib tuning results.  Again, this looks complicated, but in essense it just sets up a model training run using a Tensorflow job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705716a-46e7-4eb3-b52b-dfecb9a292c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow job operation.\n",
    "def create_tfjob_op(tfjob_name, tfjob_namespace, model, bucket, key):\n",
    "    tf_model = str(model)\n",
    "\n",
    "    tfjob_chief_spec = {\n",
    "        \"replicas\": 1,\n",
    "        \"restartPolicy\": \"OnFailure\",\n",
    "        \"template\": {\n",
    "            \"metadata\": {\n",
    "                \"annotations\": {\n",
    "                    \"sidecar.istio.io/inject\": \"false\"\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"containers\": [\n",
    "                    {\n",
    "                        \"name\": \"tensorflow\",\n",
    "                        \"image\": \"docker.io/misohu/kubeflow-training:latest\",\n",
    "                        \"command\": [\n",
    "                            \"python\",\n",
    "                            \"/opt/model.py\",\n",
    "                            \"--s3-storage=true\",\n",
    "                            f\"{tf_model}\",\n",
    "                            \"--mlflow-model-name=ml-workflow-demo-model\",\n",
    "                            f\"--bucket={bucket}\",\n",
    "                            f\"--bucket-key={key}\",\n",
    "                        ],\n",
    "                        \"envFrom\": [\n",
    "                          {\n",
    "                            \"secretRef\": {\n",
    "                              \"name\": \"mlpipeline-minio-artifact\"\n",
    "                            }\n",
    "                          }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    tfjob_worker_spec = {\n",
    "        \"replicas\": 1,\n",
    "        \"restartPolicy\": \"OnFailure\",\n",
    "        \"template\": {\n",
    "            \"metadata\": {\n",
    "                \"annotations\": {\n",
    "                    \"sidecar.istio.io/inject\": \"false\"\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"containers\": [\n",
    "                    {\n",
    "                        \"name\": \"tensorflow\",\n",
    "                        \"image\": \"docker.io/misohu/kubeflow-training:latest\",\n",
    "                        \"command\": [\n",
    "                            \"python\",\n",
    "                            \"/opt/model.py\",\n",
    "                            \"--s3-storage=true\",\n",
    "                            f\"{tf_model}\",\n",
    "                            f\"--bucket={bucket}\",\n",
    "                            f\"--bucket-key={key}\",\n",
    "                        ],\n",
    "                        \"envFrom\": [\n",
    "                          {\n",
    "                            \"secretRef\": {\n",
    "                              \"name\": \"mlpipeline-minio-artifact\"\n",
    "                            }\n",
    "                          }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the KFP task for the TFJob.\n",
    "    tfjob_launcher_op = components.load_component_from_url(\n",
    "\"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/launcher/component.yaml\")\n",
    "    op = tfjob_launcher_op(\n",
    "        name=tfjob_name,\n",
    "        namespace=tfjob_namespace,\n",
    "        chief_spec=json.dumps(tfjob_chief_spec),\n",
    "        worker_spec=json.dumps(tfjob_worker_spec),\n",
    "        tfjob_timeout_minutes=60,\n",
    "        delete_finished_tfjob=False)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc1e86",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "10. Define a complete pipeline that consists of all steps created earlier. Note that the name of the pipeline must be unique. If there was previously defined pipeline with the same name and within the same namespace either change the name of current pipeline or delete the older pipeline from the namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f18d3-1cd0-4542-9726-7e50565ad752",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = \"admin\"\n",
    "s3_bucket = \"demo-dataset\"\n",
    "key = \"data.csv\"\n",
    "dataset_url = \"https://www.openml.org/data/download/53995/KDDCup09_churn.arff\"\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name = \"ML Workflow in Kubeflow\",\n",
    "    description = \"Demo pipeline\"\n",
    ")\n",
    "def demo_pipeline(namespace=namespace):\n",
    "\n",
    "    # Step 1: Download dataset.\n",
    "    ingest_data_task = ingest_data_op(url=dataset_url)\n",
    "\n",
    "    # Step 2: Clean up the dataset and store it in S3 bucket.\n",
    "    # Note that we pass the `ingest_data_task.outputs['data']` as an argument here.  Because that output is\n",
    "    # defined as a file path, KFP knows it needs to copy the data from ingest_data_task to clean_data_task.  \n",
    "    # See [upstream docs](https://www.kubeflow.org/docs/components/pipelines/v1/sdk/python-function-components/)\n",
    "    # for more detail.\n",
    "    clean_data_task = clean_data_op(\n",
    "        s3_bucket,\n",
    "        key,\n",
    "        ingest_data_task.outputs['data']\n",
    "    )\n",
    "                       \n",
    "    # Because our S3 access needs credentials, we can apply an extra directive to pull those from an existing secret\n",
    "    # Note that this requires that the namespace you're executing the step in has this secret already\n",
    "    clean_data_task.apply(use_k8s_secret(\n",
    "        secret_name='mlpipeline-minio-artifact',\n",
    "        k8s_secret_key_to_env={\n",
    "            'accesskey': 'AWS_ACCESS_KEY_ID',\n",
    "            'secretkey': 'AWS_SECRET_ACCESS_KEY',\n",
    "        }\n",
    "    ))\n",
    "\n",
    "    # Step 3: Run hyperparameter tuning with Katib.\n",
    "    # Use the kfp.dsl.EXECUTION_ID_PLACEHOLDER to get a unique name each time we execute this pipeline\n",
    "    katib_task = create_katib_experiment_op(\n",
    "        experiment_name=f\"ml-workflow-{kfp.dsl.RUN_ID_PLACEHOLDER}\",\n",
    "        experiment_namespace=namespace,\n",
    "        bucket=s3_bucket,\n",
    "        key=key\n",
    "    )\n",
    "\n",
    "    # Our katib_task needs our cleaned data, but since we've stored that data in S3 we don't directly pass it from clean_data to katib.  \n",
    "    # Because of that, KFP does not know implicitly that katib can only be run after clean_data.  Use .after() to explicitly state this\n",
    "    # so KFP knows to schedule them in sequence.\n",
    "    katib_task.after(clean_data_task)\n",
    "    \n",
    "    # Step 4: Convert Katib results produced by hyperparameter tuning to model.\n",
    "    # Note that we do not need to use .after() here, because KFP notices best_katib_model_op needs katib_op.output.\n",
    "    best_katib_model_task = convert_katib_results_op(katib_task.output)\n",
    "\n",
    "    # Step 5: Run training with TFJob. Model will be stored into MLflow model registry\n",
    "    # (done inside container image).\n",
    "    tfjob_task = create_tfjob_op(f\"ml-workflow-{kfp.dsl.RUN_ID_PLACEHOLDER}\", namespace, best_katib_model_task.output, s3_bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59a8bf",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "11. Execute pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bb2ee-9b2b-4b81-8e28-6b86542ce531",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_client = Client()\n",
    "run_id = kfp_client.create_run_from_pipeline_func(\n",
    "        demo_pipeline,\n",
    "        namespace=namespace,\n",
    "        arguments={},\n",
    "    ).run_id\n",
    "print(f\"Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15017aa4",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "12. Observe run details by selecting **Run details** link.\n",
    "\n",
    "![Run](./images/ML-Workflow-RunDetails.png)\n",
    "\n",
    "![Pipeline](./images/ML-Workflow-Pipeline.png)\n",
    "\n",
    "13. Verify that model is stored in MLFlow model registry by navigating to MLflow dashboard, eg. http://10.64.140.43.nip.io/mlflow/#/\n",
    "\n",
    "![MLFlow](./images/ML-Workflow-MLFLowRegistry.png)\n",
    "\n",
    "14. Now model is ready to be deployed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21295c7f",
   "metadata": {},
   "source": [
    "<!--- #Execute-Pipeline --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9fcc3d702a1257c32d40ffb9a24ef331338086f3e2806172e9fc05eb0bda1b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
