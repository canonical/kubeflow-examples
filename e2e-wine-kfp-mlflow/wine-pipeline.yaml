apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: e2e-wine-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22, pipelines.kubeflow.org/pipeline_compilation_time: '2023-07-06T08:50:24.815468',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "WINE pipeline", "inputs":
      [{"name": "url"}], "name": "e2e_wine_pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22}
spec:
  entrypoint: e2e-wine-pipeline
  templates:
  - name: deploy
    container:
      args: [--model-uri, '{{inputs.parameters.training-Output}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def deploy(model_uri = "default_model_uri"):
            import subprocess

            with open("/tmp/manifest.yaml", "w") as f:
                manifest = """
        apiVersion: machinelearning.seldon.io/v1
        kind: SeldonDeployment
        metadata:
          name: mlflow
        spec:
          name: wines
          predictors:
          - componentSpecs:
            - spec:
                containers:
                - name: classifier
                  image: seldonio/mlflowserver:1.16.0
                  imagePullPolicy: Always
                  livenessProbe:
                    initialDelaySeconds: 80
                    failureThreshold: 200
                    periodSeconds: 5
                    successThreshold: 1
                    httpGet:
                      path: /health/ping
                      port: http
                      scheme: HTTP
                  readinessProbe:
                    initialDelaySeconds: 80
                    failureThreshold: 200
                    periodSeconds: 5
                    successThreshold: 1
                    httpGet:
                      path: /health/ping
                      port: http
                      scheme: HTTP
            graph:
              children: []
              implementation: MLFLOW_SERVER
              modelUri: """+model_uri+"""
              envSecretRefName: seldon-init-container-secret
              name: classifier
            name: wine-super-model
            replicas: 1
                """
                print(manifest)
                f.write(manifest)

            result = subprocess.call(["kubectl", "apply", "-f", "/tmp/manifest.yaml", "-n", "admin"])
            assert result == 0

        import argparse
        _parser = argparse.ArgumentParser(prog='Deploy', description='')
        _parser.add_argument("--model-uri", dest="model_uri", type=str, required=False, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = deploy(**_parsed_args)
      image: bponieckiklotz/seldon-deploy:0.1
    inputs:
      parameters:
      - {name: training-Output}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "model_uri"}, "then": ["--model-uri",
          {"inputValue": "model_uri"}]}}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def deploy(model_uri = \"default_model_uri\"):\n    import subprocess\n\n    with
          open(\"/tmp/manifest.yaml\", \"w\") as f:\n        manifest = \"\"\"\napiVersion:
          machinelearning.seldon.io/v1\nkind: SeldonDeployment\nmetadata:\n  name:
          mlflow\nspec:\n  name: wines\n  predictors:\n  - componentSpecs:\n    -
          spec:\n        containers:\n        - name: classifier\n          image:
          seldonio/mlflowserver:1.16.0\n          imagePullPolicy: Always\n          livenessProbe:\n            initialDelaySeconds:
          80\n            failureThreshold: 200\n            periodSeconds: 5\n            successThreshold:
          1\n            httpGet:\n              path: /health/ping\n              port:
          http\n              scheme: HTTP\n          readinessProbe:\n            initialDelaySeconds:
          80\n            failureThreshold: 200\n            periodSeconds: 5\n            successThreshold:
          1\n            httpGet:\n              path: /health/ping\n              port:
          http\n              scheme: HTTP\n    graph:\n      children: []\n      implementation:
          MLFLOW_SERVER\n      modelUri: \"\"\"+model_uri+\"\"\"\n      envSecretRefName:
          seldon-init-container-secret\n      name: classifier\n    name: wine-super-model\n    replicas:
          1\n        \"\"\"\n        print(manifest)\n        f.write(manifest)\n\n    result
          = subprocess.call([\"kubectl\", \"apply\", \"-f\", \"/tmp/manifest.yaml\",
          \"-n\", \"admin\"])\n    assert result == 0\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Deploy'', description='''')\n_parser.add_argument(\"--model-uri\",
          dest=\"model_uri\", type=str, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = deploy(**_parsed_args)\n"], "image":
          "bponieckiklotz/seldon-deploy:0.1"}}, "inputs": [{"default": "default_model_uri",
          "name": "model_uri", "optional": true, "type": "String"}], "name": "Deploy"}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"model_uri":
          "{{inputs.parameters.training-Output}}"}'}
  - name: download-data
    container:
      args: []
      command:
      - sh
      - -exc
      - |
        url="$0"
        output_path="$1"
        curl_options="$2"

        mkdir -p "$(dirname "$output_path")"
        curl --get "$url" --output "$output_path" $curl_options
      - '{{inputs.parameters.url}}'
      - /tmp/outputs/Data/data
      - --location
      image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342
    inputs:
      parameters:
      - {name: url}
    outputs:
      artifacts:
      - {name: download-data-Data, path: /tmp/outputs/Data/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml',
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"command":
          ["sh", "-exc", "url=\"$0\"\noutput_path=\"$1\"\ncurl_options=\"$2\"\n\nmkdir
          -p \"$(dirname \"$output_path\")\"\ncurl --get \"$url\" --output \"$output_path\"
          $curl_options\n", {"inputValue": "Url"}, {"outputPath": "Data"}, {"inputValue":
          "curl options"}], "image": "byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342"}},
          "inputs": [{"name": "Url", "type": "URI"}, {"default": "--location", "description":
          "Additional options given to the curl bprogram. See https://curl.haxx.se/docs/manpage.html",
          "name": "curl options", "type": "string"}], "metadata": {"annotations":
          {"author": "Alexey Volkov <alexey.volkov@ark-kun.com>", "canonical_location":
          "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml"}},
          "name": "Download data", "outputs": [{"name": "Data"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "2f61f2edf713f214934bd286791877a1a3a37f31a4de4368b90e3b76743f1523", "url":
          "https://raw.githubusercontent.com/kubeflow/pipelines/master/components/contrib/web/Download/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"Url": "{{inputs.parameters.url}}",
          "curl options": "--location"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: e2e-wine-pipeline
    inputs:
      parameters:
      - {name: url}
    dag:
      tasks:
      - name: deploy
        template: deploy
        dependencies: [training]
        arguments:
          parameters:
          - {name: training-Output, value: '{{tasks.training.outputs.parameters.training-Output}}'}
      - name: download-data
        template: download-data
        arguments:
          parameters:
          - {name: url, value: '{{inputs.parameters.url}}'}
      - name: preprocess
        template: preprocess
        dependencies: [download-data]
        arguments:
          artifacts:
          - {name: download-data-Data, from: '{{tasks.download-data.outputs.artifacts.download-data-Data}}'}
      - name: training
        template: training
        dependencies: [preprocess]
        arguments:
          artifacts:
          - {name: preprocess-output, from: '{{tasks.preprocess.outputs.artifacts.preprocess-output}}'}
  - name: preprocess
    container:
      args: [--file, /tmp/inputs/file/data, --output, /tmp/outputs/output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'pyarrow' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas' 'pyarrow' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def preprocess(
            file_path,
            output_file
        ):
            import pandas as pd
            df = pd.read_csv(file_path, header=0, sep=";")
            df.columns = [c.lower().replace(" ", "_") for c in df.columns]
            df.to_parquet(output_file)

        import argparse
        _parser = argparse.ArgumentParser(prog='Preprocess', description='')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output", dest="output_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = preprocess(**_parsed_args)
      image: python:3.9.15
    inputs:
      artifacts:
      - {name: download-data-Data, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: preprocess-output, path: /tmp/outputs/output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--file", {"inputPath": "file"}, "--output", {"outputPath": "output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas'' ''pyarrow'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' ''pyarrow''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef preprocess(\n    file_path,\n    output_file\n):\n    import
          pandas as pd\n    df = pd.read_csv(file_path, header=0, sep=\";\")\n    df.columns
          = [c.lower().replace(\" \", \"_\") for c in df.columns]\n    df.to_parquet(output_file)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Preprocess'', description='''')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output\",
          dest=\"output_file\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = preprocess(**_parsed_args)\n"], "image": "python:3.9.15"}}, "inputs":
          [{"name": "file", "type": "CSV"}], "name": "Preprocess", "outputs": [{"name":
          "output", "type": "parquet"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: training
    container:
      args: [--file, /tmp/inputs/file/data, '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'pyarrow' 'scikit-learn==1.2.2' 'mlflow==2.1.1' 'boto3' 'numpy<1.20'
        || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'pyarrow' 'scikit-learn==1.2.2' 'mlflow==2.1.1' 'boto3' 'numpy<1.20'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def training(file_path):
            import mlflow
            import pandas as pd
            from sklearn.linear_model import ElasticNet
            from sklearn.metrics import classification_report
            from sklearn.model_selection import train_test_split

            df = pd.read_parquet(file_path)

            target_column="quality"
            train_x, test_x, train_y, test_y = train_test_split(
                df.drop(columns=[target_column]),
                df[target_column], test_size=.25,
                random_state=1337, stratify=df[target_column]
            )

            mlflow.sklearn.autolog()
            with mlflow.start_run(run_name="elastic_net_models") as run:
                alpha =  0.5
                l1_ratio =  0.5
                lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)
                lr.fit(train_x, train_y)
                model_dir = "model"
                env = {
                    "name": "mlflow-env",
                    "channels": ["defaults"],
                    "dependencies": [
                        "python=3.8.10",
                        "pip"],

                    "pip":[
                        "mlflow==2.1.1",
                        "scikit-learn==0.23.2"
                    ]
                }
                mlflow.sklearn.log_model(lr, model_dir, registered_model_name="wine-elasticnet", conda_env=env)
                return f"{run.info.artifact_uri}/{model_dir}"

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Training', description='')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = training(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      env:
      - {name: MLFLOW_TRACKING_URI, value: 'http://mlflow-server.kubeflow.svc.cluster.local:5000'}
      - {name: MLFLOW_S3_ENDPOINT_URL, value: 'http://minio.kubeflow.svc.cluster.local:9000'}
      - {name: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION, value: python}
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef: {key: accesskey, name: mlpipeline-minio-artifact}
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef: {key: secretkey, name: mlpipeline-minio-artifact}
      image: python:3.8.10
    inputs:
      artifacts:
      - {name: preprocess-output, path: /tmp/inputs/file/data}
    outputs:
      parameters:
      - name: training-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: training-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--file", {"inputPath": "file"}, "----output-paths", {"outputPath":
          "Output"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas'' ''pyarrow''
          ''scikit-learn==1.2.2'' ''mlflow==2.1.1'' ''boto3'' ''numpy<1.20'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' ''pyarrow''
          ''scikit-learn==1.2.2'' ''mlflow==2.1.1'' ''boto3'' ''numpy<1.20'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def training(file_path):\n    import
          mlflow\n    import pandas as pd\n    from sklearn.linear_model import ElasticNet\n    from
          sklearn.metrics import classification_report\n    from sklearn.model_selection
          import train_test_split\n\n    df = pd.read_parquet(file_path)\n\n    target_column=\"quality\"\n    train_x,
          test_x, train_y, test_y = train_test_split(\n        df.drop(columns=[target_column]),\n        df[target_column],
          test_size=.25,\n        random_state=1337, stratify=df[target_column]\n    )\n\n    mlflow.sklearn.autolog()\n    with
          mlflow.start_run(run_name=\"elastic_net_models\") as run:\n        alpha
          =  0.5\n        l1_ratio =  0.5\n        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio,
          random_state=42)\n        lr.fit(train_x, train_y)\n        model_dir =
          \"model\"\n        env = {\n            \"name\": \"mlflow-env\",\n            \"channels\":
          [\"defaults\"],\n            \"dependencies\": [\n                \"python=3.8.10\",\n                \"pip\"],\n\n            \"pip\":[\n                \"mlflow==2.1.1\",\n                \"scikit-learn==0.23.2\"\n            ]\n        }\n        mlflow.sklearn.log_model(lr,
          model_dir, registered_model_name=\"wine-elasticnet\", conda_env=env)\n        return
          f\"{run.info.artifact_uri}/{model_dir}\"\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(\n            str(str_value),
          str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Training'', description='''')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = training(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.8.10"}}, "inputs": [{"name": "file", "type": "parquet"}],
          "name": "Training", "outputs": [{"name": "Output", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  arguments:
    parameters:
    - {name: url}
  serviceAccountName: pipeline-runner
